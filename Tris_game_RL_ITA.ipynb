{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduzione\n",
        "\n",
        "Mario Fiorino\n",
        "\n",
        "Come costruire un agente capace di giocare a Tris (Tic-Tac-Toe Game) usando un approccio basato sul Reinforcement learning?\n",
        "\n",
        "In questo notebook è stato implementato un agente Q-learning che impara a giocare a tris\n",
        "\n",
        "*Cosa si  intende per Q-learning?*\n",
        "\n",
        "Q-learning è un algoritmo  model-free, value-based, off-policy ( cioè che valuta e migliora una policy non necessariamente uguale quella utilizzata per il controllo ) temporal-difference[*] control, che consente di modellare un agente a prendere decisioni ottimali in un ambiente non necessariamente noto a priori. L'agente apprende la funzione di valore Q (che può essere una struttura dati semplice come una tabella, o qualcosa di più articolato come una rete neurale) che rappresenta il valore atteso di un'azione in un determinato stato.\n",
        "\n",
        "\n",
        "[*]NOTA:  a differenza dei metodi Monte Carlo che aspettano fino alla fine di un episodio per aggiornare le loro stime. I metodi di distanza temporale, invece, aggiornano le loro stime man mano che l'agente procede nell'episodio\n",
        "\n",
        "Ispirazione codice :\n",
        "\n",
        "https://ai.plainenglish.io/building-a-tic-tac-toe-game-with-reinforcement-learning-in-python-a-step-by-step-tutorial-5a6d9bcbb764\n",
        "\n",
        "E' stato usato come ambiente di sviluppo integrato (IDE) basato su cloud: COLAB\n",
        "\n",
        "Alcuni strumenti per la generazione di numeri random \"veri\" :\n",
        "\n",
        "https://pynative.com/python-secrets-module/\n",
        "\n",
        "https://www.geeksforgeeks.org/secrets-python-module-generate-secure-random-numbers/\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "a-feiUoCZufw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Moduli"
      ],
      "metadata": {
        "id": "XdhmE5DccWVc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "s8jJqqliXTLC"
      },
      "outputs": [],
      "source": [
        "#import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import secrets # This module is responsible for providing access to the most secure source of randomness\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Struttura del gioco Tris\n",
        "\n",
        "Il gioco del tris si gioca su una griglia 3x3, con due giocatori che si alternano nel contrassegnare una casella con il loro simbolo (X o O). L'obiettivo del gioco è ottenere tre dei propri simboli in fila, orizzontalmente, verticalmente o diagonalmente"
      ],
      "metadata": {
        "id": "VyRQ1fAIdE5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tris:\n",
        "\n",
        "    # Inizializza variabili del gioco\n",
        "    def __init__(self):\n",
        "\n",
        "        self.board = np.zeros((3, 3))\n",
        "        #  inizializza il contenuto della griglia (questo non sarà output grafico)\n",
        "        # [[0. 0. 0.]\n",
        "        #  [0. 0. 0.]\n",
        "        #  [0. 0. 0.]]\n",
        "\n",
        "        self.players = ['X', 'O']   # Due giocatori contrassegnano una casella con X oppure O. Questo punto riguarda la parte grafica\n",
        "        self.current_player = None\n",
        "        self.winner = None\n",
        "        self.game_over = False\n",
        "\n",
        "\n",
        "    # Questo metodo reimposta la tastiera di gioco, il giocatore corrente, il vincitore e\n",
        "    # lo stato di fine gioco ai loro valori iniziali.\n",
        "    def reset(self):\n",
        "        self.board = np.zeros((3, 3))\n",
        "        self.winner = None\n",
        "        self.game_over = False\n",
        "\n",
        "\n",
        "    def available_moves(self):\n",
        "        moves = []\n",
        "        for i in range(3):\n",
        "            for j in range(3):\n",
        "                if self.board[i][j] == 0:\n",
        "                    moves.append((i, j))\n",
        "        # oppure in comprehension list :\n",
        "        # moves = [(i, j) for i in range(3) for j in range(3) if self.board[i][j] == 0]\n",
        "\n",
        "        return moves   # ritorna la lista dei possibli movimenti, cioè dove le caselle sono libere ed hanno valore 0\n",
        "\n",
        "\n",
        "    def make_move(self, move):\n",
        "                      # move è una tupla con le cooridnate della casella in cui vuole effettuata la mossa.\n",
        "\n",
        "        if self.board[move[0]][move[1]] != 0:\n",
        "            return False  # Se la casella è già occupata, restituisce False, indicando che la mossa non è valida\n",
        "\n",
        "        #Aggiorna il tabellone con il simbolo del giocatore attuale,\n",
        "        #controlla se la mossa ha portato a una vittoria\n",
        "        #passa al turno dell'altro giocatore.\n",
        "        self.board[move[0]][move[1]] = self.players.index(self.current_player) + 1\n",
        "        # Note:  self.players.index(\"X\") = 0 mentre self.players.index(\"O\") = 1\n",
        "\n",
        "        self.check_winner()\n",
        "        self.switch_player()\n",
        "        return True\n",
        "\n",
        "\n",
        "    def switch_player(self):\n",
        "        if self.current_player == self.players[0]:\n",
        "            self.current_player = self.players[1]\n",
        "        else:\n",
        "            self.current_player = self.players[0]\n",
        "\n",
        "\n",
        "    def check_winner(self):\n",
        "        # Controllo riga\n",
        "        for i in range(3):\n",
        "            if self.board[i][0] == self.board[i][1] == self.board[i][2] != 0:\n",
        "                #Se c'è un vincitore, imposta di conseguenza\n",
        "                #il vincitore e\n",
        "                #lo stato di game over.\n",
        "                self.winner = self.players[int(self.board[i][0] - 1)]\n",
        "                self.game_over = True\n",
        "        # Controllo colonna\n",
        "        for j in range(3):\n",
        "            if self.board[0][j] == self.board[1][j] == self.board[2][j] != 0:\n",
        "                self.winner = self.players[int(self.board[0][j] - 1)]\n",
        "                self.game_over = True\n",
        "        # Controllo diagonali\n",
        "        if self.board[0][0] == self.board[1][1] == self.board[2][2] != 0:\n",
        "            self.winner = self.players[int(self.board[0][0] - 1)]\n",
        "            self.game_over = True\n",
        "        if self.board[0][2] == self.board[1][1] == self.board[2][0] != 0:\n",
        "            self.winner = self.players[int(self.board[0][2] - 1)]\n",
        "            self.game_over = True\n",
        "\n",
        "\n",
        "    #Output grafico della griglia\n",
        "    #Questo metodo stampa lo stato corrente della griglia\n",
        "    def print_board(self):\n",
        "        print(\"-------------\")\n",
        "        for i in range(3):\n",
        "            print(\"|\", end=' ')\n",
        "            for j in range(3):\n",
        "                print(self.players[int(self.board[i][j] - 1)] if self.board[i][j] != 0 else \" \", end=' | ')\n",
        "            print()\n",
        "            print(\"-------------\")"
      ],
      "metadata": {
        "id": "sq0rKpXhduqm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test della classe Tris\n",
        "\n",
        "Proviamo la classe appena creata, generando un'istanza della classe e giocando manualmente.  \n",
        "\n",
        "X è un utente che inserisce con input() la propria mossa.\n",
        "\n",
        "O è una funzione random.choice()\n",
        "\n",
        "In ogni iterazione del loop, richiede al giocatore corrente di inserire la propria mossa, controlla se la mossa è valida ed esegue la mossa. Dopo ogni mossa, stampa lo stato aggiornato della griglia."
      ],
      "metadata": {
        "id": "gNGVl7VccIlH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "game = Tris()\n",
        "game.current_player = game.players[0] # Imposta il giocatore corrente su X. Quindi sarà lui a far la prima mossa. Nota : game.players[0] == \"X\"\n",
        "game.print_board()\n",
        "\n",
        "while (not game.game_over) and (bool(game.available_moves())) :\n",
        "    # fin quando non ci sono vincitori : (not game.game_over) == True\n",
        "    # e\n",
        "    # fin quando non ci sono azioni possibli : (bool(game.available_moves())) == True\n",
        "    # Note : bool([]) == False\n",
        "    # stai nel loop ...\n",
        "    print(\"Azioni possibili, espresse in coordinate: \" , game.available_moves()  )\n",
        "\n",
        "    if game.current_player == game.players[0] :\n",
        "       move = input(f\"{game.current_player} è il tuo turno. Inserisci riga e colonna (e.g. 0 0): \")\n",
        "       move = tuple(map(int, move.split()))\n",
        "       # come lavora map()\n",
        "       while move not in game.available_moves():\n",
        "         move = input(\"Mossa non valida, riprova: \")\n",
        "         move = tuple(map(int, move.split()))\n",
        "    else:\n",
        "        move = random.choice(game.available_moves())\n",
        "\n",
        "    print(\"Mossa scelta : \" , move)\n",
        "\n",
        "    game.make_move(move)\n",
        "    game.print_board()\n",
        "    print(\" \")\n",
        "\n",
        "if game.winner:\n",
        "    print(f\"{game.winner} Vince!\")\n",
        "else:\n",
        "    print(\"Pareggio!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0nS1_DCdtZ5",
        "outputId": "170139c8-3860-4f83-83d5-0a236bd01306"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "Azioni possibili, espresse in coordinate:  [(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
            "X è il tuo turno. Inserisci riga e colonna (e.g. 0 0): 0 0\n",
            "Mossa scelta :  (0, 0)\n",
            "-------------\n",
            "| X |   |   | \n",
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            " \n",
            "Azioni possibili, espresse in coordinate:  [(0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
            "Mossa scelta :  (2, 2)\n",
            "-------------\n",
            "| X |   |   | \n",
            "-------------\n",
            "|   |   |   | \n",
            "-------------\n",
            "|   |   | O | \n",
            "-------------\n",
            " \n",
            "Azioni possibili, espresse in coordinate:  [(0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1)]\n",
            "X è il tuo turno. Inserisci riga e colonna (e.g. 0 0): 1 1\n",
            "Mossa scelta :  (1, 1)\n",
            "-------------\n",
            "| X |   |   | \n",
            "-------------\n",
            "|   | X |   | \n",
            "-------------\n",
            "|   |   | O | \n",
            "-------------\n",
            " \n",
            "Azioni possibili, espresse in coordinate:  [(0, 1), (0, 2), (1, 0), (1, 2), (2, 0), (2, 1)]\n",
            "Mossa scelta :  (0, 1)\n",
            "-------------\n",
            "| X | O |   | \n",
            "-------------\n",
            "|   | X |   | \n",
            "-------------\n",
            "|   |   | O | \n",
            "-------------\n",
            " \n",
            "Azioni possibili, espresse in coordinate:  [(0, 2), (1, 0), (1, 2), (2, 0), (2, 1)]\n",
            "X è il tuo turno. Inserisci riga e colonna (e.g. 0 0): 0 2\n",
            "Mossa scelta :  (0, 2)\n",
            "-------------\n",
            "| X | O | X | \n",
            "-------------\n",
            "|   | X |   | \n",
            "-------------\n",
            "|   |   | O | \n",
            "-------------\n",
            " \n",
            "Azioni possibili, espresse in coordinate:  [(1, 0), (1, 2), (2, 0), (2, 1)]\n",
            "Mossa scelta :  (1, 2)\n",
            "-------------\n",
            "| X | O | X | \n",
            "-------------\n",
            "|   | X | O | \n",
            "-------------\n",
            "|   |   | O | \n",
            "-------------\n",
            " \n",
            "Azioni possibili, espresse in coordinate:  [(1, 0), (2, 0), (2, 1)]\n",
            "X è il tuo turno. Inserisci riga e colonna (e.g. 0 0): 2 1\n",
            "Mossa scelta :  (2, 1)\n",
            "-------------\n",
            "| X | O | X | \n",
            "-------------\n",
            "|   | X | O | \n",
            "-------------\n",
            "|   | X | O | \n",
            "-------------\n",
            " \n",
            "Azioni possibili, espresse in coordinate:  [(1, 0), (2, 0)]\n",
            "Mossa scelta :  (1, 0)\n",
            "-------------\n",
            "| X | O | X | \n",
            "-------------\n",
            "| O | X | O | \n",
            "-------------\n",
            "|   | X | O | \n",
            "-------------\n",
            " \n",
            "Azioni possibili, espresse in coordinate:  [(2, 0)]\n",
            "X è il tuo turno. Inserisci riga e colonna (e.g. 0 0): 2 0\n",
            "Mossa scelta :  (2, 0)\n",
            "-------------\n",
            "| X | O | X | \n",
            "-------------\n",
            "| O | X | O | \n",
            "-------------\n",
            "| X | X | O | \n",
            "-------------\n",
            " \n",
            "X Vince!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementazione  \"Reinforcement Learning Agent\"\n",
        "\n",
        "Qui sarà implementato un agente, che tramite una tecnica di RL, l'algoritmo Q-learning, imparerà a vincere (troverà la politica ottimale per ciascuna coppia stato-azione) a Tris.\n",
        "\n"
      ],
      "metadata": {
        "id": "sQVoF4Ay00jW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearningAgent:\n",
        "    def __init__(self, alpha, epsilon, discount_factor):\n",
        "\n",
        "        self.Q = {} # la struttura dati per rappresentare la Q-table è un dizionario :\n",
        "                    #  chiave: (stato,azione) -> valore:  Q-values.\n",
        "\n",
        "        #paramentri\n",
        "        self.alpha = alpha      #  learning rate, controlla quanto i valori Q vengono aggiornati ad ogni passaggio.\n",
        "        self.epsilon = epsilon  #  exploration rate, che controlla la probabilità di scegliere un'azione casuale invece dell'azione ottimale.\n",
        "        self.discount_factor = discount_factor\n",
        "\n",
        "\n",
        "    def get_Q_value(self, state, action):\n",
        "      # Questo metodo restituisce il valore Q per una determinata coppia stato-azione.\n",
        "      # Chiavi del dizianario Q : stato, azione\n",
        "      # Ogni stato è una tupla che rappresenta lo stato corrente della griglia\n",
        "      # Esempio lo stato iniziale : [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
        "      # Ogni azione è una tupla che rappresenta le coordinate del movimento.\n",
        "      # Esempio posizione al centro della griglia : (1, 1)\n",
        "      # Valore del dizionario Q : Q-value, ovvero i valori rincompensa\n",
        "        #print(\"INPUT state in 'get_Q_value'\",state)\n",
        "        #print(\"INPUT action in 'get_Q_value'\",action)\n",
        "        if (state, action) not in self.Q:\n",
        "            self.Q[(state, action)] = 0.0   # I valori Q iniziali verranno impostati su zero\n",
        "        #print(\"OUTPUT of method 'get_Q_value',the Current dict Q=\",self.Q )\n",
        "        return self.Q[(state, action)]\n",
        "\n",
        "    def choose_action(self, state, available_moves):\n",
        "        if  secrets.SystemRandom().uniform(0,1) < self.epsilon: # oppure: random.uniform(0, 1) < self.epsilon:\n",
        "            return secrets.choice(available_moves)\n",
        "        else:\n",
        "            # sceglie l'azione con il valore Q più alto.\n",
        "            Q_values = []\n",
        "            for action in available_moves:\n",
        "                 Q_values.append(self.get_Q_value(state, action))\n",
        "            #print( \"La lista dei valori Q_values per le azioni ancora disponibili (in method 'choose_action') :\\n \",Q_values)\n",
        "            max_Q = max(Q_values)\n",
        "\n",
        "            if Q_values.count(max_Q) > 1:\n",
        "                best_moves = [i for i in range(len(available_moves)) if Q_values[i] == max_Q]\n",
        "                i = random.choice(best_moves)\n",
        "            else:\n",
        "                i = Q_values.index(max_Q)\n",
        "            return available_moves[i]\n",
        "\n",
        "    def update_Q_value(self, state, action, reward, next_state):\n",
        "        # Questo metodo aggiorna il valore Q per una determinata coppia stato-azione in base all'algoritmo Q-learning.\n",
        "\n",
        "        # Sotto la lista di tutti i valori Q che potrebbero essere ottenuti partendo dallo stato 'next_action', applicando le azioni disponibili in questo stato.\n",
        "        next_Q_values = [self.get_Q_value(next_state, next_action) for next_action in Tris().available_moves()]\n",
        "\n",
        "        if not next_Q_values : # se la lista è vuota  ritorna  0\n",
        "            max_next_Q = 0.0\n",
        "        else:\n",
        "            max_next_Q =  max(next_Q_values)    # altrimenti ritorno il massimo dei valori futuri ancora possibili\n",
        "\n",
        "        # Processo iterativo di aggiornamento e correzione basato sulla nuova informazione.\n",
        "        self.Q[(state, action)] = self.Q[(state, action)] + self.alpha * (reward + self.discount_factor * max_next_Q - self.Q[(state, action)])\n",
        "        #print(f\"Sone nel metodo 'update_Q_value'\\n Valore aggiornato di Q in stato {state} per l'azione {action} è : {self.Q[(state, action)]} \")\n",
        "\n",
        "        return self.Q"
      ],
      "metadata": {
        "id": "RsYL5-Ki00Fc"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test della classe QlearningAgent"
      ],
      "metadata": {
        "id": "BekhJPrmYHQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpha=0.5\n",
        "epsilon=0.1\n",
        "discount_factor=1.0\n",
        "\n",
        "agent = QLearningAgent(alpha, epsilon, discount_factor)\n",
        "\n",
        "state = Tris().board\n",
        "\n",
        "available_moves = Tris().available_moves()\n",
        "print(\"available_moves=\", available_moves)\n",
        "print(\"state=\\n\",state)\n",
        "\n",
        "# Per lavorare con una struttura dati dizionario\n",
        "# c'è bisogno che stato attuale della griglia sia hashable e non un 'numpy.ndarray'\n",
        "boardHash = str(state.reshape(3 * 3))\n",
        "print(\"boardHash=\",boardHash)\n",
        "\n",
        "action = agent.choose_action(boardHash, available_moves)\n",
        "print(\"action=\",action)\n",
        "\n",
        "state[action[0],action[1]] = 1\n",
        "print(\" state after action (or next_state) =\\n\",state)\n",
        "next_state = state\n",
        "\n",
        "w = True\n",
        "if w : # se c'è un vincitore, vedi def check_winner\n",
        "  reward = 1\n",
        "else :\n",
        "  reward = 0\n",
        "\n",
        "next_boardHash = str(next_state.reshape(3 * 3))\n",
        "dq = agent.update_Q_value(boardHash, action, reward, next_boardHash)\n",
        "\n",
        "sorted_dq = sorted(dq.items(), key=lambda x:x[1], reverse=True)\n",
        "dq = dict(sorted_dq)\n",
        "print(dq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rg0-46LOYQeo",
        "outputId": "1d6b2290-7648-4413-a50c-916509c20450"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "available_moves= [(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
            "state=\n",
            " [[0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]]\n",
            "boardHash= [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "La lista dei valori Q_values per le azioni ancora disponibili (in method 'choose_action') :\n",
            "  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "action= (1, 2)\n",
            " state after action (or next_state) =\n",
            " [[0. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 0.]]\n",
            "Metodo 'update_Q_value'\n",
            " Valore aggiornato di Q in stato [0. 0. 0. 0. 0. 0. 0. 0. 0.] per l'azione (1, 2) è : 0.5 \n",
            "{('[0. 0. 0. 0. 0. 0. 0. 0. 0.]', (1, 2)): 0.5, ('[0. 0. 0. 0. 0. 0. 0. 0. 0.]', (0, 0)): 0.0, ('[0. 0. 0. 0. 0. 0. 0. 0. 0.]', (0, 1)): 0.0, ('[0. 0. 0. 0. 0. 0. 0. 0. 0.]', (0, 2)): 0.0, ('[0. 0. 0. 0. 0. 0. 0. 0. 0.]', (1, 0)): 0.0, ('[0. 0. 0. 0. 0. 0. 0. 0. 0.]', (1, 1)): 0.0, ('[0. 0. 0. 0. 0. 0. 0. 0. 0.]', (2, 0)): 0.0, ('[0. 0. 0. 0. 0. 0. 0. 0. 0.]', (2, 1)): 0.0, ('[0. 0. 0. 0. 0. 0. 0. 0. 0.]', (2, 2)): 0.0, ('[0. 0. 0. 0. 0. 1. 0. 0. 0.]', (0, 0)): 0.0, ('[0. 0. 0. 0. 0. 1. 0. 0. 0.]', (0, 1)): 0.0, ('[0. 0. 0. 0. 0. 1. 0. 0. 0.]', (0, 2)): 0.0, ('[0. 0. 0. 0. 0. 1. 0. 0. 0.]', (1, 0)): 0.0, ('[0. 0. 0. 0. 0. 1. 0. 0. 0.]', (1, 1)): 0.0, ('[0. 0. 0. 0. 0. 1. 0. 0. 0.]', (1, 2)): 0.0, ('[0. 0. 0. 0. 0. 1. 0. 0. 0.]', (2, 0)): 0.0, ('[0. 0. 0. 0. 0. 1. 0. 0. 0.]', (2, 1)): 0.0, ('[0. 0. 0. 0. 0. 1. 0. 0. 0.]', (2, 2)): 0.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fase di Training"
      ],
      "metadata": {
        "id": "GFAvd7LWbi7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "game = Tris()\n",
        "# Ricorda gli stati possibili nel gioco sono 3^9 = 19683\n",
        "# 9 caselle\n",
        "# vuoto, marcato X, marcato O.\n",
        "\n",
        "num_episodes = 80_000\n",
        "\n",
        "alpha=0.5\n",
        "epsilon=0.05\n",
        "discount_factor=1.0\n",
        "\n",
        "game.current_player = game.players[0]\n",
        "#              Nota : game.players[0] == \"X\"  è il nostro agente e farà la prima mossa\n",
        "\n",
        "\n",
        "agent = QLearningAgent(alpha, epsilon, discount_factor)\n",
        "\n",
        "\n",
        "for i in range(1,num_episodes + 1):\n",
        "     print(\"\\nEpisode nr : \", i)\n",
        "\n",
        "     game.reset() # pulisce la griglia dalle operazioni precedenti per poter iniziare un nuovo episodio\n",
        "     state = game.board\n",
        "     stateHash = str(state.reshape(3 * 3)) # Ricorda : c'è bisogno che stato attuale della griglia sia hashable e non un 'numpy.ndarray'\n",
        "\n",
        "     #game.print_board()\n",
        "\n",
        "     # The exploration-exploitation trade-off\n",
        "     # Qui un sempllice tunning per bilanciare esplorazione e sfruttamento\n",
        "     if i > 40000:\n",
        "        epsilon=0.5\n",
        "     elif i > 70000:\n",
        "        epsilon=0.8\n",
        "\n",
        "     while (not game.game_over) and (bool(game.available_moves())) :\n",
        "\n",
        "         #STEP 1\n",
        "         #Seleziona un’azione possibile in un certo stato ed eseguila\n",
        "         print(\"\\n***Muova mossa****\\n\")\n",
        "         print(\"Azioni possibili : \" , game.available_moves()  )\n",
        "\n",
        "         print(\"Sta muovendo il giocatore : \",game.current_player)\n",
        "         if game.current_player == game.players[0] :\n",
        "              move = agent.choose_action(stateHash, game.available_moves())\n",
        "              #print(\"Azione scelta dal Q_learning_Agent=\",move)\n",
        "         else:\n",
        "              move = secrets.choice(game.available_moves())\n",
        "\n",
        "         game.make_move(move)\n",
        "\n",
        "         # Stampa a schermo la situazione di gioco della griglia dopo l'azione 'move'\n",
        "         # game.print_board()\n",
        "\n",
        "         # STEP 2\n",
        "         # agente riceve la ricompensa\n",
        "         # game.winner puo essere \"X\" o \"O\"\n",
        "         if game.winner == 'X':\n",
        "            print(f\"\\t\\t{game.winner} Vince!\")\n",
        "            reward = 1.0\n",
        "         elif game.winner == 'O':\n",
        "            print(f\"\\t\\t{game.winner} Vince!\")\n",
        "            reward = - 0.05\n",
        "         else:\n",
        "            reward = 0.0\n",
        "\n",
        "         # STEP 3\n",
        "         #Qui si costruisce ed aggiorna la struttura dati  Q\n",
        "         next_state = game.board   # 'next_state' è il nuovo stato in cui si arriva eseguendo azione: 'action'\n",
        "         next_boardHash = str(next_state.reshape(3 * 3))\n",
        "\n",
        "\n",
        "         dq = agent.update_Q_value(stateHash, move, reward, next_boardHash)\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nSTRUTTURA DATI Q_TABLE (in pratica un dict) ottenuta alla fine del training\")\n",
        "#print(dq)\n",
        "print(\"Dimensoni : \",len(dq))\n",
        "\n",
        "#new = pd.DataFrame.from_dict(dq,orient ='index')\n",
        "#print(\"\\n\")\n",
        "#new\n",
        "\n",
        "#\n",
        "# Salva la struttura dati ottenuta su un file :\n",
        "#\n",
        "with open('Tris_data_train.pkl', 'wb') as fp:\n",
        "    pickle.dump(dq, fp)\n",
        "    print('dictionary saved successfully to file')"
      ],
      "metadata": {
        "id": "Mn59csy7e6dD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Riprendi i dati del file salvati"
      ],
      "metadata": {
        "id": "kLzzbVS4MHS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "with open('Tris_data_train.pkl', 'rb') as fp:\n",
        "    dq_in_file = pickle.load(fp)\n",
        "\n",
        "print(len(dq_in_file))\n",
        "#dp = dq_in_file"
      ],
      "metadata": {
        "id": "QuKIx2RKK92A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test 1 delle prestazioni dell'agente Q-learning\n",
        "\n",
        "Dopo aver addestrato l'agente Q-learning, possiamo testarne le prestazioni prima contro un giocatore umano."
      ],
      "metadata": {
        "id": "e4741cjYsHgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "game = Tris()\n",
        "\n",
        "game.current_player = game.players[0] # Nota : game.players[0] == \"X\", imposta il giocatore corrente su X e sarà lui a fare la prima mossa\n",
        "# In questo caso, game.players[0] è il Q_learning_Agent\n",
        "#Alternativa,\n",
        "#ogni giocatore ha il 50% di possibilità di essere il primo:\n",
        "#game.current_player = random.choice(game.players)\n",
        "\n",
        "#game.print_board()\n",
        "\n",
        "while (not game.game_over) and (bool(game.available_moves())) :\n",
        "\n",
        "    state = game.board\n",
        "    stateHash = str(state.reshape(3 * 3))\n",
        "\n",
        "    print(\"Azioni possibili, espresse in coordinate: \" , game.available_moves()  )\n",
        "\n",
        "    if game.current_player == game.players[1] :\n",
        "       move = input(f\"{game.current_player} è il tuo turno. Inserisci riga e colonna (e.g. 0 0): \")\n",
        "       move = tuple(map(int, move.split()))\n",
        "       while move not in game.available_moves():\n",
        "         move = input(\"Mossa non valida, riprova: \")\n",
        "         move = tuple(map(int, move.split()))\n",
        "\n",
        "       # Se si volesse lavorare con un agente random non umano usare:\n",
        "       # move = secrets.choice(game.available_moves())\n",
        "\n",
        "    else:\n",
        "        # Agente_Q ha in input lo state e sceglie l'azione con Q-value più altro\n",
        "\n",
        "        # Sotto costruisco un sotto-dizionario contente le sole azioni disponibili per un certo stato dato in input\n",
        "        sub_dq = {}\n",
        "        for i in game.available_moves():\n",
        "          #print(i)\n",
        "          sub_dq.update({(stateHash,i) : dq[stateHash,i]})\n",
        "\n",
        "        newdq = pd.DataFrame.from_dict(sub_dq,orient ='index')\n",
        "        print(\"Sub-dict delle azioni disponibili in un certo stato : \\n \",newdq)\n",
        "\n",
        "        # ne ricavo la key del dizionario che ha value Q massimo\n",
        "        index_max_Qvalue = max(sub_dq, key=sub_dq.get)\n",
        "        print(\"Indice con value Q massimo : \",index_max_Qvalue )\n",
        "\n",
        "        # Recupera l'azione da fare\n",
        "        move =  index_max_Qvalue[1]\n",
        "        #print(move)\n",
        "\n",
        "\n",
        "    game.make_move(move)\n",
        "    game.print_board()\n",
        "    print(\" \\nFINE MOSSA\\n \")\n",
        "\n",
        "\n",
        "if game.winner:\n",
        "    print(f\"{game.winner} Vince!\")\n",
        "else:\n",
        "    print(\"Pareggio!\")"
      ],
      "metadata": {
        "id": "ZOKLVXA7NW2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test 2 delle prestazioni dell'agente Q-learning\n",
        "\n",
        "Possiamo testarne le prestazioni contro un giocatore-agente random."
      ],
      "metadata": {
        "id": "SPI0TY3HEcYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_games = 10_000\n",
        "\n",
        "num_wins = 0\n",
        "\n",
        "game = Tris()\n",
        "\n",
        "game.current_player = random.choice(game.players)\n",
        "\n",
        "\n",
        "for j in range(num_games):\n",
        "\n",
        "        game.reset()\n",
        "        state = game.board\n",
        "        stateHash = str(state.reshape(3 * 3))\n",
        "\n",
        "        while (not game.game_over) and (bool(game.available_moves())) :\n",
        "\n",
        "             if game.current_player == game.players[1] :\n",
        "              # Agente_Random\n",
        "                  move = secrets.choice(game.available_moves())\n",
        "\n",
        "             else:\n",
        "              # Agente_Q\n",
        "                sub_dq = {}\n",
        "                for i in game.available_moves():\n",
        "                  sub_dq.update({(stateHash,i) : dq[stateHash,i]})\n",
        "\n",
        "                newdq = pd.DataFrame.from_dict(sub_dq,orient ='index')\n",
        "                #print(\"Sub-dict delle azioni disponibili in un certo stato : \\n \",newdq)\n",
        "\n",
        "\n",
        "                index_max_Qvalue = max(sub_dq, key=sub_dq.get)\n",
        "                move =  index_max_Qvalue[1]\n",
        "\n",
        "             game.make_move(move)\n",
        "             #game.print_board()\n",
        "\n",
        "        #print(\"Fine partita nr : \", j )\n",
        "        #print(\"WINNER : \",  game.winner)\n",
        "\n",
        "        if game.winner == game.players[0] :\n",
        "           num_wins += 1\n",
        "\n",
        "print(\"\\nESITO FINALE DEL TEST :\")\n",
        "print(num_wins / num_games * 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-sgtZcKEbhG",
        "outputId": "0ee3cc43-1a2d-4616-b85f-179ea25a79e5"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ESITO FINALE DEL TEST :\n",
            "69.04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusione\n",
        "\n",
        "Usando la struttura dati ottenuta da questo veloce addestramento: 'Tris_data_train.pkl'; il nostro agente Q_learning vince quasi il 70% delle volte contro un agente_random"
      ],
      "metadata": {
        "id": "NPBQ5JgkK1yc"
      }
    }
  ]
}