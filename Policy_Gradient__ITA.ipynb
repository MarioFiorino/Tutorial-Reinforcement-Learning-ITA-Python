{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "W-D7yGghOtof",
        "PCQDs_HibFNI",
        "M5NKE1oDoxAJ",
        "iKBt4S-CCeqM"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Policy Based Methods\n",
        "\n",
        "Mario Fiorino\n",
        "\n",
        "$\\;$\n",
        "\n",
        "**Introduzione**\n",
        "\n",
        "I metodi presentati nei notebook precedenti (\"action-value methods\") si basano sulla funzione $Q(s,a)$ valore-azione. Ovvero:\n",
        "\n",
        "- Si apprendere la funzione $Q(s,a)$ attraverso l'interazione con l'ambiente.  \n",
        "\n",
        "- Una volta appresa tale la funzione, l'agente seleziona l'azione da intraprendere nello stato corrente $s$ basandosi sui valori stimati di $Q(s,a)$. Tipicamente, viene scelta l'azione che massimizza il valore stimato.\n",
        "\n",
        "-  Tali metodi non esplicitano una funzione policy$: a → s $, questa è implicitamente definita dai valori di $Q(s,a)$.\n",
        "\n",
        "$\\;$\n",
        "\n",
        "I metodi policy based invece:\n",
        "\n",
        "- Apprendono una funzione di policy, parametrizzata; ad esempio una ANN.\n",
        "\n",
        "- Così strutturate, tale funzione di policy in grado di selezionare azioni senza consultare una funzione $Q(s,a)$\n",
        "\n",
        "\n",
        "\n",
        "$\\;$\n",
        "\n",
        "Vantaggi\n",
        "\n",
        "- La policy parametrizzata, in genere, è una funzione più semplice da approssimare, rispetto alla parametrizzazione della funzione valore-azione.\n",
        "\n",
        "- Stochastic policies : questi metodi semplificano le cosa quando si tratta di apprendere policy che selezionano azioni in modo probabilistico (utili in scenari con incertezza). Ad esempio nel gioco Sasso-carta-forbici(Rock-Paper-Scissors), la policy ottimale da ricavare è di tipo stocastico.\n",
        "\n",
        "- Efficacia in \"high-dimensional\" spazi ed/o in spazi ad azione continua.\n",
        "\n",
        "- Infine, la parametrizzazione della policy è un buon modo per sfruttare la conoscenza pregressa nella definizione della forma desiderata della policy.\n",
        "\n",
        "Svantaggi:\n",
        "\n",
        "\n",
        "- Tali metodi tendono a convergere verso un locale ottimo, piuttosto che verso l'ottimo globale.\n",
        "\n",
        "- La valutazione della performance di una policy può essere inefficente e presentare un'alta varianza."
      ],
      "metadata": {
        "id": "7XIrltRMwtbO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Idee di base\n",
        "\n"
      ],
      "metadata": {
        "id": "zmDpyzWy_uk9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Notazioni:\n",
        "\n",
        "\n",
        "$\\textbf{θ}$ è il vettore dei parametri della policy $\\pi_\\theta$ , con dimensioni $d'$. Ovvero : $\\textbf{θ} \\in R^{d'}$\n",
        "\n",
        "$\\pi(a|s, \\textbf{θ}) = Pr \\{ A_t =a | S_t =s, \\textbf{θ}_t = \\textbf{θ} \\} \\; \\; $  indica la probabilità che l'azione $a$ venga intrapresa nello stato $s$ al tempo $t$, con i parametri della policy uguali $\\textbf{θ}$.\n",
        "\n",
        "$J(\\textbf{θ})$ è una funzione, spesso chiamata **objective function** (funzione obiettivo) , che misura le performance policy ${\\pi_\\theta}$, dipendente dalla variazione dei suoi parametri. Ad esempio negli ambienti episodici, un modo molto semplice per valutare tale rendimento, è quello di scegliere la value fuction della policy nello stato di partenza, cioè expected return ottenuto seguendo tale policy,  partendo da uno specifico stato iniziale :\n",
        "$J(\\textbf{θ}) = V_{\\pi_\\theta} (s_{0}) = \\mathbb{E}_{ {\\pi_\\theta}} [ \\sum_{t=0}^{T} r_t | s_0] $ .\n",
        "Si noti, in questo esempio si assume che il discount factor $\\gamma = 1 $ e si considera un numero finito di episodi. $r_t$ è il reward ottenuto ad ogni step seguendo una certa traiettoria di eventi stato-azione campionata da $\\pi_\\theta$. $T$ è lo step nel Terminal state.\n",
        "\n",
        "Sia  $\\tau$ un traiettoria di eventi stato-azione campionata da $\\pi_\\theta$ che parte da un stato $s_0$, allora\n",
        "\n",
        "$  \\mathbb{E}_{ {\\pi_\\theta}} [ \\sum_{t=0}^{T} r_t | s_0] = \\mathbb{E}_{\\tau\\sim \\pi_θ} [R(\\tau)] = \\int_{\\tau} P(\\tau|\\pi_\\theta) R(\\tau) $\n",
        "\n",
        "Dove $R(\\tau)$ è la somma dei premi ottenuti da $\\tau$.\n",
        "Mentre $P(\\tau|\\pi_\\theta)$ è la probabilità di estrarre $\\tau$ dalla policy $\\pi_\\theta$\n",
        "\n",
        "$\\;$\n",
        "\n",
        "**Obiettivo**:\n",
        "\n",
        "Apprendere i parametri $\\textbf{θ}$ che massimizzano $J(\\textbf{θ})$.\n",
        "\n",
        "$\\;$\n",
        "\n",
        "Come?\n",
        "\n",
        "Per risolvere tale problema, si possono impiegare diverse strategie, le più comuni si suddividono in due categorie:\n",
        "\n",
        "Metodi non basati sul gradiente:\n",
        "\n",
        "- Hill climbing (Questo metodo esplora lo spazio dei parametri in modo iterativo, muovendosi nella direzione che aumenta i valori della funzione $J$)\n",
        "\n",
        "- Simplex / amoeba / Nelder Mead\n",
        "\n",
        "- Genetic algorithms\n",
        "\n",
        "Metodi basati sul gradiente:\n",
        "\n",
        "- Gradient descent\n",
        "\n",
        "- Conjugate gradient\n",
        "\n",
        "- Quasi-newton\n",
        "\n",
        "\n",
        "In questo notebook, ci concentriamo specificamente sul gradient descent (discesa del gradiente), data la sua ampia diffusione e la sua efficacia in molti scenari.\n"
      ],
      "metadata": {
        "id": "7QMy3PYTFCe5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Policy Gradient\n",
        "\n",
        "\n",
        "$\\;$\n",
        "\n",
        "\n",
        "L'idea di fondo dei metodi policy gradient è quello di ottimizzare la policy del sistema incrementando la probabilità di azioni che conducono a rendimenti più elevati, e diminuendo la probabilità di azioni associate a rendimenti inferiori.\n",
        "\n",
        "$\\;$\n",
        "\n",
        "\n",
        "Data una certa distribuzione $\\rho$ da cui sono campionati gli stati iniziali $s_0$; consideriamo la seguente funzione obiettivo, che sia una funzione differenziabile in $\\mathbf{θ}$:\n",
        "\n",
        "$J(\\textbf{θ}) =  \\mathbb{E}_{ {s_0 \\sim \\rho}} [ V_{\\pi_\\theta} (s_0)] $\n",
        "\n",
        "Gli algoritmi policy gradient si prefiggono di individuare quei valori ottimali dei parametri $\\mathbf{θ^{*}}$ che definiscono la policy $\\pi_\\mathbf{θ}$, con l'obiettivo di massimizzare la funzione di valore $J(\\mathbf{θ})$, spesso raggiungendo un massimo locale.\n",
        "\n",
        "Per realizzare ciò, questi algoritmi impiegano la tecnica del *stochastic gradient ascent*, aggiornando così i parametri:\n",
        "\n",
        "$\\mathbf{\\theta_{k+1}} = \\mathbf{\\theta_k} + \\alpha \\nabla_{\\mathbf{\\theta}} J(\\mathbf{\\theta_k}) \\; \\;$ $\\; \\;$  Nota Formale, limitazioni di LaTeX nel notebook: se pur nell'equazione esposta, i parametri $\\mathbf{{\\theta_k}}$ non sono rappresentati in grassetto, sono sempre da pensarsi come vettori di dimensione $d'$.\n",
        "\n",
        "$\\;$\n",
        "\n",
        "**Problema** : Come ricavare  $\\nabla_{\\theta} J(\\textbf{θ})$ ?\n",
        "\n",
        "Il calcolo del gradiente è complicato perché, sia la selezione dell'azione ( questa è la parte fattibile ) sia la distribuzione  degli stati in cui vengono effettuate tali selezioni (qui ci sono delle complicazioni in quanto tale distribuzione dipende sia dalla policy che dal modello dell’ambiente, che è tipicamente sconosciuto in un contesto RL model-free) dipendono dalle variazioni dei parametri $\\textbf{θ}$ della policy.\n",
        "\n",
        "\n",
        "In pratica, poiché la probabilità delle varie traiettorie ottenute dalla policy $\\pi_{\\mathbf{θ}}$ dipende anche dal modello incognito dell'ambiente, non possiamo calcolare direttamente il gradiente $\\nabla_{\\theta} J(\\mathbf{θ})$.\n",
        "\n",
        "$\\;$"
      ],
      "metadata": {
        "id": "XKGVY4WNPoR1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Soluzione: **Policy Gradient Theorem**\n",
        "\n",
        "Il teorema Policy Gradient fornisce un'espressione analitica computazionalmente fattibile del gradiente in questione:\n",
        "\n",
        "\n",
        "$\\nabla_{\\theta} J(\\mathbf{θ}) \\propto  \\mathbb{E}_{ {s \\sim d^{\\pi_\\theta} _{\\rho}} ; \\;a \\sim {\\pi_\\theta} } [  Q_{\\pi_{θ}}(s,a) \\nabla_{\\theta} \\log \\pi_{θ}(a|s )] $\n",
        "\n",
        "\n",
        "Dove:\n",
        "\n",
        "Il simbolo $\\propto$ significa “proporzionale a”.\n",
        "\n",
        "$d^{\\pi_\\theta} _{s_0} (s)$ è la *state visitation distribution* che praticamente descrivie la probabilità (o frequenza) di finire in un certo stato $s$, partendo da uno stato iniziale $s_0$, è seguendo la policy $\\pi_\\theta$. Per maggiori info (Equivalent Formulations of Policies and Reward) : https://www.alexirpan.com/rl-derivations/\n",
        "\n",
        "$d^{\\pi_\\theta} _{\\rho}  = \\mathbb{E}_{s_0 \\sim ρ} [ d^{\\pi_\\theta} _{s_0} (s)]$\n",
        "\n",
        "$\\;$\n",
        "\n",
        "Per la verifica analitico-matematica del teorema:\n",
        "Sutton & Barto,2018, PDF 347\n",
        "\n",
        "oppure: https://users.ece.cmu.edu/~yuejiec/ece18813B_notes/lecture12-policy-optimization.pdf\n",
        "\n",
        "\n",
        "$\\;$\n",
        "\n",
        "In pratica, il teorema esprime una relazione proporzionale tra il gradiente della policy $\\nabla_{\\theta} \\pi(a|s)$ (un vettore colonna che possiamo campionare) ed il gradiente $ \\nabla_{\\theta} J(\\mathbf{θ})$.\n",
        "\n",
        "Si noti:\n",
        "\n",
        "Così espresso, il gradiente di $J$ può essere rappresentato come expectation $\\mathbb{E}[ \\; ]$. Significa che possiamo usare un campionamento per approssimarlo.\n",
        "\n",
        "\n",
        "\n",
        "$\\;$\n",
        "\n",
        "\n",
        "\n",
        "Formulazione Generale:\n",
        "\n",
        "Sia $X$ una variabile casuale, e data una funzione di densità di probabilità  $p_θ(X)$. Si definisce funzione obiettivo $J(θ)$:\n",
        "\n",
        "\n",
        "$J(θ)=\\mathbb{E}[f(X)]=∫_x f(x)p_{θ} (x)dx$\n",
        "\n",
        "\n",
        "Per $f(x)$ funzione arbitraria che non dipende da $θ$\n",
        "\n",
        "\n",
        "Vogliamo calcolare $∇_θ J(θ)$. Con delle manipolazioni matematiche, possiamo cambiare il gradiente dell'expectation nell'expectation del gradiente:\n",
        "\n",
        "$ ∇_θ J(θ)=∫_x f(x) ∇_θ p_θ(x) dx = \\mathbb{E}[f(x)∇_θ \\log(p_θ(x) )]$\n",
        "\n",
        "Supponiamo ora che la variabile casuale sia una traiettoria $τ=(s_0,a_0,s_1,a_1,s_2,⋯)$, generata dalla policy $π_θ$ in un MDP. Sia $f(τ)$ il return lungo tale traiettoria. $J(θ)$ è ora l'expected return della policy, ed il suo gradiente offre un modo per aggiornare la policy verso una maggiore ricompensa.\n",
        "\n",
        "Si osservi infine, che avremmo potuto scegliere anche un'altra funzione obiettivo, diversa da $ \\mathbb{E}_{ {s_0 \\sim \\rho}} [ V_{\\pi_\\theta} (s_0)] $, il risultato finale del Policy Gradient Theorem è indipendente da questa scelta.\n",
        "\n",
        "\n",
        "$\\;$\n",
        "\n",
        "Ref:\n",
        "\n",
        "\n",
        "\n",
        "Compendio Policy Gradient Algorithms, chiaro e completo:\n",
        "\n",
        "https://lilianweng.github.io/posts/2018-04-08-policy-gradient/\n",
        "\n",
        "\n",
        "Corso di David Silver:\n",
        "\n",
        "https://www.youtube.com/watch?v=KHZVXao4qXs\n",
        "\n",
        "\n",
        "Elliot Waite - Machine learning video; un ottima spiegazione con un esempio pratico:\n",
        "\n",
        "https://www.youtube.com/watch?v=cQfOQcpYRzE\n",
        "\n",
        "\n",
        "\n",
        "Sutton & Barto, Reinforcement learning, An Introduction, 2018.\n",
        "\n",
        "https://jonathan-hui.medium.com/rl-policy-gradients-explained-9b13b688b146"
      ],
      "metadata": {
        "id": "xa_xEdL5g9Rl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##REINFORCE (aka Monte Carlo Policy Gradient)\n",
        "\n",
        "\n",
        "REINFORCE = REward Increment = Non-negative Factor × Offset Reinforcement × Characteristic Eligibility\n",
        "\n",
        "\n",
        "L'algoritmo è stato introdotto da Ronald J. Williams nel 1992.\n",
        "\n",
        "\n",
        "\n",
        "$\\;$\n",
        "\n",
        "Idee chiave:\n",
        "\n",
        "- Aggiornare i parametri tramite la tecnica stochastic gradient-ascent, sfruttando il teorema Policy Gradient per ricavare $\\nabla_{\\theta} J(\\mathbf{θ})$\n",
        "\n",
        "- Viene impiegato il classico Return completo Monte Carlo, $G_t$:  cioè a partire dallo step $t$, tale valore include tutte le ricompense future ottenute dalla policy fino al termine dell'episodio. Nella formulazione ottenuta dal teorema Policy Gradient, questo $G_t$ sostituisce $Q_{\\pi}(s,a)$\n",
        "\n",
        "\n",
        "$\\;$\n",
        "\n",
        "Regola di update dei parametri della policy:\n",
        "\n",
        "$\\mathbf{\\theta}_{t+1} = \\theta_t + \\alpha \\cdot [ G_t\\frac{\\nabla \\pi (A_t|S_t, \\theta_t)}{\\pi(A_t|S_t, \\theta_t)} ] \\; \\;$ Si ricorda che $\\theta$ se pur non in grassetto, è da considerarsi un vettore; e che la notazione $π$  è uno snellimento di  $π_θ$.\n",
        "\n",
        "Si noti, $\\nabla_{\\theta} J({θ}) = \\mathbb{E}_{\\pi} [ G_t \\frac{\\nabla \\pi (A_t|S_t, \\theta_t)}{\\pi(A_t|S_t, \\theta_t)} ] $. In parantesi quadra vi è una quantità campionabile ad ogni time-step la cui expectation è uguale al gradiente della funzione obiettivo.\n",
        "\n",
        "Sfruttando la legge analitica di derivazione del logaritmo , l'espressione frazionaria viene sostituita da\n",
        "\n",
        "$∇ \\log \\pi (A_t|S_t, \\theta_t) =  \\frac{\\nabla \\pi (A_t|S_t, \\theta_t)}{\\pi(A_t|S_t, \\theta_t)} $\n",
        "\n",
        "Questa formulazione logaritmica risutla anche computazionalmente più vantaggiosa.\n",
        "\n",
        "\n",
        "$\\;$\n",
        "\n",
        "Approfondimenti Teorici\n",
        "\n",
        "Per un approfondimento sui passaggi algebrici e matematici che portano a questa regola, consultare il testo di Sutton & Barto, 2018, PDF 348 - 349. Sempre in queste pagine viene evidenziata la modifica usata nell'update: $[ G_t\\frac{\\nabla \\pi (A_t|S_t, \\theta_t)}{\\pi(A_t|S_t, \\theta_t)} ]$ , rispetto la formulazione teorica di base che prevede una sommatoria sulle azioni:\n",
        "*This algorithm, which has been called an all-actions method because its update involves all of the actions, is promising and deserving of further study, but our current interest is the classical REINFORCE algorithm\n",
        "(Willams, 1992) whose update at time t involves just $A_t$, the one action actually taken at time t.*\n",
        "\n",
        "Sempre il testo di Sutton & Barto, aggiunge il seguente commento finale riguardo tale formulazione:\n",
        "\n",
        "*This update has an intuitive appeal. Each increment is proportional to the product of a return $G_t$ and a vector, the gradient of the probability of taking the action actually taken divided by the probability of taking that action. The vector is the direction in parameter space that most increases the probability of repeating the action $A_t$ on future visits to state $S_t$. The update increases the parameter vector in this direction proportional to the return, and inversely proportional to the action probability. The former makes sense because it causes the parameter to move most in the directions that favor actions that yield the highest return. The latter makes sense because otherwise actions that are selected frequently are at an advantage (the updates will be more often in their direction) and might win out even if they do not yield the highest return.*\n",
        "\n",
        "$\\;$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Convergenza**\n",
        "\n",
        "Per valori di $α$ sufficentemente piccoli,\n",
        "è garantito un miglioramento delle prestazioni della policy nella massimizzazione della ricompensa.\n",
        "\n",
        "La convergenza verso un ottimo locale è garantita con le condizioni di approssimazione stocastica (stochastic approximation conditions) standard, viste più volte in questi notebook, per $α$ decrescente.\n",
        "\n",
        "\n",
        "$\\;$\n",
        "\n",
        "\n",
        "**Svantaggi**\n",
        "\n",
        "\n",
        "Come per il metodo Monte Carlo classico, REINFORCE può presentare un'elevata varianza e quindi produrre un apprendimento lento.\n",
        "\n",
        "Di fattto abbiamo bisogno di ulteriori meccanismi per ridurre la varianza. Un esempio di risposta a tale problema :\n",
        "\n",
        "\n",
        "\n",
        "REINFORCE with Baseline (Sutton & Barto, 2018, PDF 351)\n",
        "\n"
      ],
      "metadata": {
        "id": "4nrDVHLiZeK2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Env: Cart Pole (Gym)\n",
        "\n",
        "$\\;$\n",
        "\n",
        "Nota : In CartPole, l'agente riceve una ricompensa con un valore pari a $+1$ per ogni singolo time-step in cui riesce a mantenere l'equilibrio .\n",
        "\n",
        "https://www.gymlibrary.dev/environments/classic_control/cart_pole/\n",
        "\n",
        "\n",
        "https://github.com/openai/gym/wiki/Table-of-environments\n"
      ],
      "metadata": {
        "id": "W-D7yGghOtof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "\n",
        "env=gym.make('CartPole-v1')\n",
        "#help(env.unwrapped)\n",
        "\n",
        "episodeNumber=2\n",
        "max_timeSteps=3\n",
        "\n",
        "\n",
        "for episodeIndex in range(1,episodeNumber):\n",
        "    initial_state=env.reset()\n",
        "\n",
        "    #print(\"Ep:\",episodeIndex)\n",
        "    #appendedObservations=[]\n",
        "\n",
        "    for timeIndex in range(1,max_timeSteps+1):\n",
        "        print(\"timeIndex=\",timeIndex)\n",
        "\n",
        "        random_action=env.action_space.sample()\n",
        "        print(\"random_action=\",random_action)\n",
        "\n",
        "        observation, reward, done,  info = env.step(random_action)\n",
        "\n",
        "        #appendedObservations.append(observation)\n",
        "\n",
        "        print(\"[Cart Position,Cart Velocity,Pole Angle,Pole Angular Velocity]:\",observation)\n",
        "        print(\"reward=\",reward)\n",
        "        print(\"done=\",done)\n",
        "        print(\"info=\",info)\n",
        "        print(\"\")\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xovkHfpOvek",
        "outputId": "57bda386-2e60-470f-98f1-68b97cbf90f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "timeIndex= 1\n",
            "random_action= 0\n",
            "[Cart Position,Cart Velocity,Pole Angle,Pole Angular Velocity]: [-0.03882672 -0.17139114  0.04385891  0.34949616]\n",
            "reward= 1.0\n",
            "done= False\n",
            "info= {}\n",
            "\n",
            "timeIndex= 2\n",
            "random_action= 1\n",
            "[Cart Position,Cart Velocity,Pole Angle,Pole Angular Velocity]: [-0.04225454  0.02308048  0.05084883  0.07095963]\n",
            "reward= 1.0\n",
            "done= False\n",
            "info= {}\n",
            "\n",
            "timeIndex= 3\n",
            "random_action= 0\n",
            "[Cart Position,Cart Velocity,Pole Angle,Pole Angular Velocity]: [-0.04179293 -0.1727322   0.05226803  0.37924212]\n",
            "reward= 1.0\n",
            "done= False\n",
            "info= {}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Moduli ed Inizializzazione ambiente"
      ],
      "metadata": {
        "id": "PCQDs_HibFNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = (16, 10)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "torch.manual_seed(0)\n",
        "\n",
        "import base64, io\n",
        "\n",
        "# For visualization\n",
        "from gym.wrappers.monitoring import video_recorder\n",
        "from IPython.display import HTML\n",
        "from IPython import display\n",
        "import glob\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device\n",
        "\n",
        "env = gym.make('CartPole-v1')"
      ],
      "metadata": {
        "id": "x82A8OrsML9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Premesse tecniche alla Policy come ANN\n",
        "\n",
        "La **funzione softmax** trasforma un vettore di numeri reali in una distribuzione di probabilità.\n",
        "In pratica, applica una funzione esponenziale a ciascun elemento, rendendolo positivo, e poi divide per la somma di tutti i valori esponenzializzati, garantendo che la somma delle probabilità risulti pari a uno.\n",
        "\n",
        "In questo scenario, dove si sono solo due azioni discrete (le nostre \"classi\"), la funzione softmax produce una distribuzione di probabilità su queste due azioni.\n",
        "\n",
        "Per spazi di azioni discreti, la funzione softmax rappresenta una scelta comune."
      ],
      "metadata": {
        "id": "M5NKE1oDoxAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Funzione softmax in Pytorch\n",
        "# dim (int) – A dimension along which Softmax will be computed (so every slice along dim will sum to 1)\n",
        "\n",
        "softmax0 = torch.nn.Softmax(dim=0) # Applies along columns\n",
        "softmax1 = torch.nn.Softmax(dim=1) # Applies along rows\n",
        "\n",
        "v = np.array([[1,2,3],\n",
        "              [1,5,0]])\n",
        "v =  torch.from_numpy(v).float()\n",
        "\n",
        "print(softmax0(v))\n",
        "print(\"\")\n",
        "print(softmax1(v))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dFHXbyk25jl",
        "outputId": "11f1a4d2-b12f-4ec4-d72f-24c50b1867cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5000, 0.0474, 0.9526],\n",
            "        [0.5000, 0.9526, 0.0474]])\n",
            "\n",
            "tensor([[0.0900, 0.2447, 0.6652],\n",
            "        [0.0179, 0.9756, 0.0066]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Categorical in pytorch\n",
        "\n",
        "#La classe Categorical in PyTorch definisce una distribuzione di probabilità discreta.\n",
        "#Puoi specificare la distribuzione in due modi (ma non contemporaneamente):\n",
        "#Probabilità (probs): Tramite un tensore contenente le probabilità relative di ciascuna categoria. La somma degli elementi di questo tensore deve essere pari a 1 (lungo la dimensione specificata durante il campionamento).\n",
        "#Logit (logits): Tramite un tensore contenente i logit per ciascuna categoria.\n",
        "\n",
        "# Se probs è unidimensionale con lunghezza K, ciascun elemento rappresenta la probabilità relativa di campionare la categoria corrispondente all'indice.\n",
        "# ovvero i campioni saranno interi da {0,…,K−1}\n",
        "\n",
        "#https://pytorch.org/docs/stable/distributions.html\n",
        "\n",
        "#log_prob\n",
        "#Restituisce semplicemente il logaritmo della probabilità che si verifichi un determinato campione (o categoria) estratto dalla distribuzione.\n",
        "#https://en.wikipedia.org/wiki/Log_probability\n",
        "#https://chrispiech.github.io/probabilityForComputerScientists/en/part1/log_probabilities/\n",
        "\n",
        "print(\"E1\")\n",
        "m = Categorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))\n",
        "s = m.sample()\n",
        "print(s)  # ha un uguale probabilità di campionare 0, 1, 2, 3\n",
        "print(m.log_prob(s))\n",
        "print(m.log_prob(torch.from_numpy(np.ones(1)*3)))\n",
        "\n",
        "print(\"E2\")\n",
        "m = Categorical(torch.tensor([ 0.9999, 0.00001 ]))\n",
        "s = m.sample()\n",
        "print(s)  # quasi sempre campiona 0\n",
        "print(m.log_prob(s))\n",
        "print(m.log_prob(torch.from_numpy(np.ones(1))))\n",
        "\n",
        "print(\"E3\")\n",
        "m = Categorical(torch.tensor([ 0.33, 0.33, 0.33, 0.01 ]))\n",
        "s = m.sample()\n",
        "print(s)  # di rado campiona 3\n",
        "print(m.log_prob(s))\n",
        "print(m.log_prob(torch.from_numpy(np.ones(1)*3)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgiKyfKo5kPx",
        "outputId": "b96b3101-78bc-4562-c172-322f3d0501b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E1\n",
            "tensor(0)\n",
            "tensor(-1.3863)\n",
            "tensor([-1.3863])\n",
            "E2\n",
            "tensor(0)\n",
            "tensor(-1.0014e-05)\n",
            "tensor([-11.5128])\n",
            "E3\n",
            "tensor(1)\n",
            "tensor(-1.1087)\n",
            "tensor([-4.6052])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Policy come ANN\n",
        "\n",
        "\n",
        "\n",
        "I metodi basati sulle policy, producono in uscita una certa distribuzione di probabilità sulle azioni disponibili. Ad esempio, se si hanno solo due azioni discrete in un certo stato, potrebbe concretizzarzi in uscita una distribuzione del tipo $[0.2 \\;\\; 0.8]$.\n",
        "\n",
        "\n",
        "$\\;$\n",
        "\n",
        "Si noti: per garantire l’esplorazione e quindi l'apprendimento, si richiede che la policy non diventi mai puramente deterministica.\n"
      ],
      "metadata": {
        "id": "TlFj5UzVbIR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Codice ispirato :\n",
        "#https://github.com/goodboychan?tab=repositories\n",
        "\n",
        "class Policy(nn.Module):\n",
        "\n",
        "    #Definiamo una ANN a due layer fully connected con relative activation function\n",
        "    def __init__(self, state_size=4, action_size=2, hidden_size=32):\n",
        "        super(Policy, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, hidden_size) # Matrice pesi fc1.weight: torch.Size([32, 4])\n",
        "        self.activation1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, action_size) # Matrice pesi fc2.weight: torch.Size([2, 32])\n",
        "        self.activation2 = torch.nn.Softmax(dim=1) # Nota per dim=0 da chiaramente [[1., 1.]]\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = self.fc1(state)\n",
        "        x1 = self.activation1(x)\n",
        "        x2 = self.fc2(x1) #è una roba del tipo tensor([[-0.0344,  0.0977]], grad_fn=<AddmmBackward0>)\n",
        "        x3 = self.activation2(x2)\n",
        "        return x3\n",
        "\n",
        "    # Definiamo un modo per campionare un azione e il log della sua probabilità\n",
        "    def act(self, state):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device) # questo rigo evita il TypeError:\n",
        "        # linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray\n",
        "\n",
        "        probs = self.forward(state).cpu()\n",
        "        model = Categorical(probs)\n",
        "        action = model.sample() # qui campiona l'azione usando i risultati della ANN\n",
        "        return action.item(), model.log_prob(action)"
      ],
      "metadata": {
        "id": "tcwqe0gIbH6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing della classe\n",
        "\n",
        "p = Policy().to(device)\n",
        "state = env.reset()\n",
        "\n",
        "for timeIndex in range(1,2):\n",
        "\n",
        "        state1 = torch.from_numpy(state).float().unsqueeze(0)\n",
        "        print(\"p.forward(state)=\", p.forward(state1))\n",
        "\n",
        "        action, log_prob = p.act(state)\n",
        "        print(\"action=\",action)\n",
        "        print(\"log_prob=\",log_prob)\n",
        "\n",
        "        state, reward, done, _ = env.step(action)\n",
        "\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99e1c770-fc16-4a10-eb6b-210f7540284e",
        "id": "UUfSutVMwR6l"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p.forward(state)= tensor([[0.5816, 0.4184]], grad_fn=<SoftmaxBackward0>)\n",
            "action= 0\n",
            "log_prob= tensor([-0.5419], grad_fn=<SqueezeBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Premesse alla REINFORCE Core function\n"
      ],
      "metadata": {
        "id": "iKBt4S-CCeqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Premssa Tecnica : torch.cat\n",
        "#https://pytorch.org/docs/stable/generated/torch.cat.html\n",
        "\n",
        "x = torch.randn(1, )\n",
        "print(x)\n",
        "y = torch.randn(1, )\n",
        "print(y)\n",
        "\n",
        "print(torch.cat([x,y],dim=0))\n",
        "print( (torch.cat([x,y],dim=0))[0].type)\n",
        "print(\"sum=\", torch.cat([x,y]).sum())\n",
        "print(\"\")\n",
        "\n",
        "####\n",
        "\n",
        "x = torch.randn(1,4 )\n",
        "print(x)\n",
        "print(torch.cat([x, x, x],dim=1))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIShOYzZHfoo",
        "outputId": "8ca250db-028b-4866-f3b4-422d6bb8ca76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.3001])\n",
            "tensor([1.3226])\n",
            "tensor([0.3001, 1.3226])\n",
            "<built-in method type of Tensor object at 0x798f25ca7330>\n",
            "sum= tensor(1.6227)\n",
            "\n",
            "tensor([[-1.2141, -2.2789, -2.6911,  0.0192]])\n",
            "tensor([[-1.2141, -2.2789, -2.6911,  0.0192, -1.2141, -2.2789, -2.6911,  0.0192,\n",
            "         -1.2141, -2.2789, -2.6911,  0.0192]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Buono a sapersi:\n",
        "\n",
        "Anche se non è il caso di Cart Pole, dove tutti i rewads sono uguali (e non verrà qui applicato); generalmente, per questioni di stabilità, si preferisce normalizzare il return dei rewards. Se si analizza le equazioni di backpropagation, si osserva che il return influenza i gradienti. Per tale ragione, si vuole mantenere i valori campionati del return, in un intervallo specifico.\n",
        "\n",
        "L' adozione di questa normalizzazione non si basa su solide garanzie teoriche, bensì su considerazioni di natura pratica."
      ],
      "metadata": {
        "id": "Xtw5eSbZ3kVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_rewards(r):\n",
        "  # r è la lista contenente i rewads accumulati dell'episodio\n",
        "  r = torch.tensor(r).to(device)\n",
        "  print(r.mean())\n",
        "  print(r.std())\n",
        "  r = (r - r.mean()) / (r.std() + 1e-9)\n",
        "  return r # ritorno la lista di reward normalizzati"
      ],
      "metadata": {
        "id": "hLi2D03v3gKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### REINFORCE Core function"
      ],
      "metadata": {
        "id": "Mz_O9I7R4K-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def list_G_for_each_step(input_list, gamma=1):\n",
        "    output_list = []\n",
        "    # Itera in reverse order\n",
        "    for i in range(len(input_list) - 1, -1, -1):\n",
        "        # Calcola la somma\n",
        "        current_sum = sum(input_list[i:])\n",
        "        output_list.append(current_sum)\n",
        "        # NOTA: semplifichiamo le cose assumendo che gamma = 1.\n",
        "        # Altrimenti nell'elaborazione avremmo dovuto considerare anche\n",
        "        # il fattore discount:  *gamma**(k-t-1))\n",
        "        # Vedi algoritmo di Sutton&Barto PDF 350\n",
        "    output_list.reverse()\n",
        "    return output_list\n",
        "\n",
        "\n",
        "def reinforce(policy, optimizer, n_episodes=1000, max_t=1000, gamma=1.0, print_every=100):\n",
        "\n",
        "    scores_deque = deque(maxlen=100)\n",
        "    scores = []\n",
        "\n",
        "    for e in range(1, n_episodes+1):\n",
        "        saved_log_probs = []\n",
        "        rewards = []\n",
        "        state = env.reset()\n",
        "\n",
        "        # Genera una traiettoria\n",
        "        for t in range(max_t):\n",
        "            action, log_prob = policy.act(state)\n",
        "            saved_log_probs.append(log_prob) # è roba del tipo : log_prob= tensor([-0.7196])\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            rewards.append(reward)\n",
        "            #la lista reward cresce in dimensione ad ogni step: rewards [1.0], rewards [1.0, 1.0], rewards [1.0, 1.0, 1.0], ...\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "\n",
        "        #Elabora il return G_t\n",
        "        G = list_G_for_each_step(rewards)\n",
        "        #R = sum(rewards)  # un alternativa praticabile\n",
        "\n",
        "        # Utility\n",
        "        scores_deque.append(sum(rewards)) # Serve per il debug\n",
        "        scores.append(sum(rewards)) # Serve per il grafico\n",
        "\n",
        "        # Calcola la loss\n",
        "        policy_loss = []\n",
        "        for j in range(len(G)):\n",
        "            # Tieni presente che stiamo utilizzando Gradient Ascent, non il Descent.\n",
        "            # E' una convenzione standard, sia in PyTorch che TF, eseguire la minimizzazione anziché la massimizzazione.\n",
        "            # Quindi, dobbiamo convertire l'obiettivo da massimizzazione in obiettivo da minimizzazione,\n",
        "            # semplicemente aggiungendo un segno negativo.\n",
        "            policy_loss.append(-saved_log_probs[j] * G[j])\n",
        "\n",
        "        # Varainte algoritmo standard\n",
        "        # A differenza dell'algortimo proposto da Sutton PDF 350\n",
        "        # dove update dei parametri è fatto \"for each step of the episode\"\n",
        "        # in questo caso  preferisco fare un unico update per ogni episodio\n",
        "        # sfruttando la somma sulle (log_prod*G) campionate.\n",
        "        # Per la versione standard suggerisco:\n",
        "        # https://github.com/mitchellvitez/vanilla-policy-gradient/tree/main\n",
        "        #\n",
        "        # Nota: torch.cat(policy_loss).mean()\n",
        "        # usando tale istruzione le cose non funzionano,\n",
        "        # ottengo valori ballerini Average Score: 75.22,.. 493.66, 177.89 , ... , 91.63\n",
        "        # e risulta nei test : \"Max reward was taken in  0.0 % of times\"\n",
        "        #\n",
        "        # Nota : torch.cat\n",
        "        # Si tenga presente che torch.cat trasforma la lista classica di tensori in un tensore \"lista\" che contiene tensori\n",
        "        # cioè, da: policy_loss= [tensor([12.8589], grad_fn=<MulBackward0>), tensor([12.5127], grad_fn=<MulBackward0>), tensor([8.6259],..]\n",
        "        # a:  policy_loss= tensor([12.8589, 12.5127, 8.6259, ...,  12.4928])\n",
        "\n",
        "        policy_loss = torch.cat(policy_loss).sum()\n",
        "        #policy_loss è una roba del tipo : tensor(10.4294, grad_fn=<MeanBackward0>)\n",
        "\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        # Debug\n",
        "        if e % print_every == 0:\n",
        "            print('Episode {}\\tAverage Score: {:.2f}'.format(e, np.mean(scores_deque)))\n",
        "\n",
        "        # Se raggiungi un certo punteggio, esempio 195.0, ferma il processo di training\n",
        "        #if np.mean(scores_deque) >= 195.0:\n",
        "        #    print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(e - 100, np.mean(scores_deque)))\n",
        "        #    break\n",
        "\n",
        "    print(\"\\n...End of the training.\\n\")\n",
        "    return scores"
      ],
      "metadata": {
        "id": "73f5baxkDBTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "dbvhfbCIelAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.autograd.set_detect_anomaly(True)\n",
        "#env.spec.reward_threshold = 600  # the standard threshold for rewards is 500 for v1\n",
        "\n",
        "policy = Policy().to(device)\n",
        "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
        "score = reinforce(policy, optimizer, n_episodes=1_000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_2MwvsXLBy7",
        "outputId": "7327b5f8-ce52-4315-cd6a-2632b115a4c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 100\tAverage Score: 53.81\n",
            "Episode 200\tAverage Score: 204.24\n",
            "Episode 300\tAverage Score: 266.08\n",
            "Episode 400\tAverage Score: 153.34\n",
            "Episode 500\tAverage Score: 281.38\n",
            "Episode 600\tAverage Score: 258.84\n",
            "Episode 700\tAverage Score: 255.41\n",
            "Episode 800\tAverage Score: 422.23\n",
            "Episode 900\tAverage Score: 494.36\n",
            "Episode 1000\tAverage Score: 500.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Plot"
      ],
      "metadata": {
        "id": "hkJBCeeEjwdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the scores\n",
        "fig = plt.figure(figsize=(4,2))\n",
        "#ax = fig.add_subplot(111)\n",
        "ax = plt.gca()\n",
        "ax.set_ylim([1, 600])\n",
        "plt.plot(np.arange(1, len(score)+1), score)\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Episode')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "m_8UQxeSjqa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test con Video"
      ],
      "metadata": {
        "id": "jprU4k4QkLc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_video(env_name):\n",
        "    mp4list = glob.glob('./*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = './{}.mp4'.format(env_name)\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        display.display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "def show_video_of_model(policy, env_name):\n",
        "    env = gym.make(env_name)\n",
        "    vid = video_recorder.VideoRecorder(env, path=\"./{}.mp4\".format(env_name))\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    for t in range(1000):\n",
        "        vid.capture_frame()\n",
        "        action, _ = policy.act(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        state = next_state\n",
        "        if done:\n",
        "            break\n",
        "    vid.close()\n",
        "    env.close()"
      ],
      "metadata": {
        "id": "7RO2VEqokRxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_video_of_model(policy, 'CartPole-v1')"
      ],
      "metadata": {
        "id": "_qO1FxNNkVTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_video('CartPole-v1')"
      ],
      "metadata": {
        "id": "zNvm98ziLJ0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test su 100 campioni"
      ],
      "metadata": {
        "id": "wBaxTF2BnY0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "episodeNumber=100\n",
        "max_timeSteps=1_000\n",
        "\n",
        "cont = 0\n",
        "max_reward = 500\n",
        "\n",
        "\n",
        "for episodeIndex in range(1,episodeNumber):\n",
        "    observation=env.reset()\n",
        "    rewards = 0\n",
        "\n",
        "    for timeIndex in range(1,max_timeSteps+1):\n",
        "        action, _ = policy.act(observation)\n",
        "        observation, reward, done,  info = env.step(action)\n",
        "        rewards += reward\n",
        "\n",
        "        if done:\n",
        "            if max_reward == rewards:\n",
        "                  cont += 1\n",
        "\n",
        "            break\n",
        "\n",
        "env.close()\n",
        "\n",
        "print(\"Max reward was taken in \", round((cont/episodeNumber)*episodeNumber,2), \"% of times\" )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jO-HG2hknesH",
        "outputId": "6cf642e1-61ee-4f29-ceca-803b510b262b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max reward was taken in  99.0 % of times\n"
          ]
        }
      ]
    }
  ]
}