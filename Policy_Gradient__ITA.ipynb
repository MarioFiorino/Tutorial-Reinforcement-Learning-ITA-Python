{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "W-D7yGghOtof",
        "M5NKE1oDoxAJ",
        "TlFj5UzVbIR2",
        "iKBt4S-CCeqM"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Policy Based Methods\n",
        "\n",
        "Mario Fiorino\n",
        "\n",
        "$\\;$\n",
        "\n",
        "**Introduzione**\n",
        "\n",
        "I metodi presentati nei notebook precedenti (\"action-value methods\") si basano sulla funzione $Q(s,a)$ valore-azione. Ovvero:\n",
        "\n",
        "- Si apprendere la funzione $Q(s,a)$ attraverso l'interazione con l'ambiente.  \n",
        "\n",
        "- Una volta appresa tale la funzione, l'agente seleziona l'azione da intraprendere nello stato corrente $s$ basandosi sui valori stimati di $Q(s,a)$. Tipicamente, viene scelta l'azione che massimizza il valore stimato.\n",
        "\n",
        "-  Tali metodi non esplicitano una funzione policy$: a → s $, questa è implicitamente definita dai valori di $Q(s,a)$.\n",
        "\n",
        "$\\;$\n",
        "\n",
        "I metodi policy based invece:\n",
        "\n",
        "- Apprendono una funzione di policy, parametrizzata; ad esempio una ANN.\n",
        "\n",
        "- Così strutturate, tale funzione di policy in grado di selezionare azioni senza consultare una funzione $Q(s,a)$\n",
        "\n",
        "\n",
        "\n",
        "$\\;$\n",
        "\n",
        "Vantaggi\n",
        "\n",
        "- La policy parametrizzata, in genere, è una funzione più semplice da approssimare, rispetto alla parametrizzazione della funzione valore-azione.\n",
        "\n",
        "- Stochastic policies : questi metodi semplificano le cosa quando si tratta di apprendere policy che selezionano azioni in modo probabilistico (utili in scenari con incertezza). Ad esempio nel gioco Sasso-carta-forbici(Rock-Paper-Scissors), la policy ottimale da ricavare è di tipo stocastico.\n",
        "\n",
        "- Efficacia in \"high-dimensional\" spazi ed/o in spazi ad azione continua.\n",
        "\n",
        "- Infine, la parametrizzazione della policy è un buon modo per sfruttare la conoscenza pregressa nella definizione della forma desiderata della policy.\n",
        "\n",
        "Svantaggi:\n",
        "\n",
        "\n",
        "- Tali metodi tendono a convergere verso un locale ottimo, piuttosto che verso l'ottimo globale.\n",
        "\n",
        "- La valutazione della performance di una policy può essere inefficente e presentare un'alta varianza."
      ],
      "metadata": {
        "id": "7XIrltRMwtbO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Idee di base\n",
        "\n"
      ],
      "metadata": {
        "id": "zmDpyzWy_uk9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Notazioni:\n",
        "\n",
        "\n",
        "$\\textbf{θ}$ è il vettore dei parametri della policy, con dimensioni $d'$. Ovvero : $\\textbf{θ} \\in R^{d'}$\n",
        "\n",
        "$\\pi(a|s, \\textbf{θ}) = Pr \\{ A_t =a | S_t =s, \\textbf{θ}_t = \\textbf{θ} \\} \\; \\; $  indica la probabilità che l'azione $a$ venga intrapresa nello stato $s$ al tempo $t$, con i parametri della policy uguali $\\textbf{θ}$.\n",
        "\n",
        "$J(\\textbf{θ})$ è una funzione, spesso chiamata **objective function**, che misura le performance policy, dipendente dai suoi parametri. Ad esempio negli ambienti episodici possiamo scegliere il valore della value fuction nello stato di partenza della policy ${\\pi_\\theta}$ seguita: $J(\\textbf{θ}) = V_{\\pi_\\theta} (s_{0}) = \\mathbb{E}_{ {\\pi_\\theta}} [ \\sum_{t=0}^{T} r_t | s_0] $  \n",
        "\n",
        "Si noti, in questo caso si assume che il discount factor $\\gamma = 1 $ e si considera un numero finito di episodi. $r_t$ è il reward ottenuto ad ogni step seguendo una certa traiettoria di eventi stato-azione campionata da $\\pi_\\theta$. $T$ è lo step nel Terminal state.\n",
        "\n",
        "Convenzioni di notazione: sia  $\\tau$ un traiettoria di eventi stato-azione campionata da $\\pi_\\theta$ che parte da un stato $s_0$, allora\n",
        "\n",
        "$  \\mathbb{E}_{ {\\pi_\\theta}} [ \\sum_{t=0}^{T} r_t | s_0] = \\mathbb{E}_{\\tau\\sim \\pi_θ} [R(\\tau)] = \\int_{\\tau} P(\\tau|\\pi_\\theta) R(\\tau) $\n",
        "\n",
        "Dove $R(\\tau)$ è la somma dei premi ottenuti da $\\tau$.\n",
        "\n",
        "Mentre $P(\\tau|\\pi_\\theta)$ è la probabilità di estrarre $\\tau$ dalla policy $\\pi_\\theta$\n",
        "\n",
        "$\\;$\n",
        "\n",
        "Obiettivo:\n",
        "\n",
        "Apprendere i parametri $\\textbf{θ}$ che massimizzano $J(\\textbf{θ})$.\n",
        "\n",
        "$\\;$\n",
        "\n",
        "Come?\n",
        "\n",
        "Per risolvere tale problema, si possono impiegare diverse strategie, le più comuni si suddividono in due categorie:\n",
        "\n",
        "Metodi non basati sul gradiente:\n",
        "\n",
        "- Hill climbing (Questo metodo esplora lo spazio dei parametri in modo iterativo, muovendosi nella direzione che aumenta i valori della funzione $J$)\n",
        "\n",
        "- Simplex / amoeba / Nelder Mead\n",
        "\n",
        "- Genetic algorithms\n",
        "\n",
        "Metodi basati sul gradiente:\n",
        "\n",
        "- Gradient descent\n",
        "\n",
        "- Conjugate gradient\n",
        "\n",
        "- Quasi-newton\n",
        "\n",
        "\n",
        "In questo notebook, ci concentriamo specificamente sul gradient descent (discesa del gradiente), data la sua ampia diffusione e la sua efficacia in molti scenari.\n"
      ],
      "metadata": {
        "id": "7QMy3PYTFCe5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Policy Gradient\n",
        "\n",
        "\n",
        "$\\;$\n",
        "\n",
        "\n",
        "L'idea di fondo dei metodi policy gradient è quello di ottimizzare la policy del sistema incrementando la probabilità di azioni che conducono a rendimenti più elevati, e diminuendo la probabilità di azioni associate a rendimenti inferiori.\n",
        "\n",
        "\n",
        "$\\;$\n",
        "\n",
        "\n",
        "Premesse:\n",
        "\n",
        "Data, o melgio inizializzati i parametri di una certa policy  $\\pi_\\mathbf{θ} (a|s)$, ovvero una funzione che in ingresso prende lo stato corrente ed in uscita produce uan certa distribuzione di probabilità sulle azioni disponibili. Ad esempio, se si hanno solo due azioni discrete in un certo stato $s$, potrebbe essere concretizzata con una distribuzione del tipo $[0.2 \\;\\; 0.8]$. Invece, in uno spazio di azione continua (Continuous action space), dove l'agente può selezionare un'azione da un intervallo di valori continui per ciascuno stato ( ad esempio, nella guida di un auto: selezionare una velocità in un intervallo che va da 0,75 m/s a 4 m/s,  ed un angolo di sterzata da -20 a 20 gradi); la scelta più comune per concretizzare $\\pi_\\mathbf{θ} (a|s)$ è una funzione gaussiana: volendo una rete neurale il cui input sono le features dello stato ed in unscita produce due valori : un media ed una deviazione standard.\n",
        "\n",
        "Si noti: per garantire l’esplorazione e quindi l'apprendimento, si richiede che la policy non diventi mai puramente deterministica.\n",
        "\n",
        "Consideriamo la seguente funzione obiettivo, che sia una funzione differenziabile in $\\mathbf{θ}$:\n",
        "\n",
        "$J(\\mathbf{θ}) = \\sum_s \\mu_{\\pi}(s) \\cdot V_{\\pi} (s)$\n",
        "\n",
        "Nota Formale: al fine di snellire l'equazioni, adotteremo questa sezione \"Policy Gradient\",  in pedice, la notazione $\\pi$ per indicadere $\\pi_\\mathbf{θ}$, cioè dipendente dai parametri $\\mathbf{θ}$.\n",
        "\n",
        "\n",
        "$\\mu_{\\pi}(s)$ indica la distribuzione degli stati data la policy $\\pi$, ovvero indica la probabilità di finire in un certo stato $s$, seguendo la policy $\\pi$. Per maggiori dettagli consultare il testo Sutton & Barto, 2018, PDF 221.\n",
        "\n",
        "\n",
        "Si tenga anche presente che $ V_{\\pi} (s) = \\sum_{a} \\pi(a|s)Q_{\\pi}(s,a) $\n",
        "\n",
        "\n",
        "Gli algoritmi policy gradient si prefiggono di individuare i valori ottimali dei parametri $\\mathbf{θ^{*}}$ che definiscono la policy, con l'obiettivo di massimizzare la funzione di valore $J(\\mathbf{θ})$, spesso raggiungendo un massimo locale. Una sintesi in simboli:\n",
        "$\\mathbf{θ^{*}} =  \\arg \\max_{\\mathbf{θ}} J(\\mathbf{θ})$\n",
        "\n",
        "\n",
        "\n",
        "Lo fanno con la tecnica del *stochastic gradient ascent*, di fatto aggiornando i parametri in questo modo:\n",
        "\n",
        "$\\mathbf{\\theta_{k+1}} = \\mathbf{\\theta_k} + \\alpha \\nabla_{\\mathbf{\\theta}} J(\\mathbf{\\theta_k}) \\; \\;$ $\\; \\;$  Nota Formale, limitazioni di LaTeX nel notebook: se pur nell'equazione esposta, i parametri $\\mathbf{{\\theta_k}}$ non sono rappresentati in grassetto, sono sempre da pensarsi come vettori di dimensione $d'$.\n",
        "\n",
        "\n",
        "$\\;$\n",
        "\n",
        "\n",
        "Come ricavare  $\\nabla_{\\theta} J(\\textbf{θ})$ ?\n",
        "\n",
        "Il calcolo del gradiente è complicato perché dipende sia dalla selezione dell'azione (determinata direttamente dalla parametrizzaione della $π(a|s)$, ma questo è un calcolo facile) sia dalla distribuzione  degli stati in cui vengono effettuate tali selezioni (qui ci sono delle complicazioni in quanto tale distribuzione dipende sia dalla policy che dal modello dell’ambiente, che è tipicamente sconosciuto in un contesto RL model-free).\n",
        "\n",
        "\n",
        "In pratica, poiché la probabilità delle varie traiettorie ottenute dalla policy $\\pi$ dipende anche dal modello incognito dell'ambiente, non possiamo calcolare il gradiente $\\nabla_{\\theta} J(\\mathbf{θ})$ rispetto ai parametri $\\mathbf{θ}$ .\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "$\\;$\n",
        "\n",
        "Soluzione: **Policy Gradient Theorem**\n",
        "\n",
        "Il teorema Policy Gradient fornisce un'espressione analitica per il gradiente rispetto ai parametri della  policy che non coinvolge la derivata della distribuzione degli stati:\n",
        "\n",
        "\n",
        "$\\nabla_{\\theta} J(\\mathbf{θ}) \\propto \\sum_s \\mu_{\\pi}(s) \\sum_{a} Q_{\\pi}(s,a) \\nabla_{\\theta} \\pi(a|s;\\mathbf{θ} ) $\n",
        "\n",
        "Dove il simbolo $\\propto$ significa “proporzionale a”. Nel caso episodico la costante di proporzionalità è la\n",
        "durata media di un episodio, e nel caso continuato è 1, per cui il rapporto è effettivamente di uguaglianza.\n",
        "\n",
        "\n",
        "Dimostrazione: Sutton & Barto,2018, PDF 347\n",
        "e di utile aiuto: https://lilianweng.github.io/posts/2018-04-08-policy-gradient/\n",
        "\n",
        "\n",
        "\n",
        "Così espresso, il gradiente può essere rappresentato come expectation $\\mathbb{E}$. Significa che possiamo usare il campionamento per approssimarlo.\n",
        "\n",
        "Si noti, campioniamo la distribuzione degli stati $\\mu_{\\pi}(s)$ ottenuta seguendo una certa policy, ma non la differenziamo.\n",
        "\n",
        "In pratica, il teorema esprime una relazione proporzionale tra il gradiente della policy $\\nabla_{\\theta} \\pi(a|s)$ (un vettore colonna che possiamo campionare) ed il gradiente della funzione $ \\nabla_{\\theta} J(\\mathbf{θ})$.\n",
        "\n",
        "$\\;$\n",
        "\n",
        "Ref:\n",
        "\n",
        "\n",
        "Corso di David Silver:\n",
        "\n",
        "https://www.youtube.com/watch?v=KHZVXao4qXs\n",
        "\n",
        "\n",
        "Elliot Waite - Machine learning video; un ottima spiegazione con un esempio pratico:\n",
        "\n",
        "https://www.youtube.com/watch?v=cQfOQcpYRzE\n",
        "\n",
        "\n",
        "\n",
        "Sutton & Barto, Reinforcement learning, An Introduction, 2018.\n",
        "\n",
        "https://jonathan-hui.medium.com/rl-policy-gradients-explained-9b13b688b146\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XKGVY4WNPoR1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##REINFORCE (aka Monte Carlo Policy Gradient)\n",
        "\n",
        "\n",
        "REINFORCE = REward Increment = Non-negative Factor × Offset Reinforcement × Characteristic Eligibility\n",
        "\n",
        "\n",
        "L'algoritmo è stato introdotto da Ronald J. Williams nel 1992.\n",
        "\n",
        "\n",
        "\n",
        "$\\;$\n",
        "\n",
        "Idee chiave:\n",
        "\n",
        "- Aggiornare i parametri tramite la tecnica stochastic gradient-ascent, sfruttando il teorema Policy Gradient per ricavare $\\nabla_{\\theta} J(\\mathbf{θ})$\n",
        "\n",
        "- Viene impiegato il classico Return completo Monte Carlo, $G_t$:  cioè a partire dallo step $t$, tale valore include tutte le ricompense future ottenute dalla policy fino al termine dell'episodio. Nella formulazione ottenuta dal teorema Policy Gradient, questo $G_t$ sostituisce $Q_{\\pi}(s,a)$\n",
        "\n",
        "\n",
        "$\\;$\n",
        "\n",
        "Regola di update dei parametri della policy:\n",
        "\n",
        "\n",
        "\n",
        "$\\mathbf{\\theta}_{t+1} = \\theta_t + \\alpha [ G_t\\frac{\\nabla \\pi (A_t|S_t, \\theta_t)}{\\pi(A_t|S_t, \\theta_t)} ] \\; \\;$ Si ricorda che $\\theta$ se pur non in grassetto, è da considerarsi un vettore; e che la notazione $π$  è uno snellimento di  $π_θ$.\n",
        "\n",
        "Si noti, $\\nabla_{\\theta} J({θ}) = \\mathbb{E}_{\\pi} [ G_t \\frac{\\nabla \\pi (A_t|S_t, \\theta_t)}{\\pi(A_t|S_t, \\theta_t)} ] $. In parantesi quadra vi è una quantità campionabile ad ogni time-step la cui expectation è uguale al gradiente della funzione obiettivo.\n",
        "\n",
        "Per un approfondimento sui passaggi algebrici e matematici che portano a questa regola, consultare il testo di Sutton & Barto, 2018, PDF 349.\n",
        "\n",
        "Sempre il testo di Sutton & Barto, aggiunge il seguente commento riguardo tale formulazione:\n",
        "\n",
        "*This update has an intuitive appeal. Each increment is proportional to the product of a return $G_t$ and a vector, the gradient of the probability of taking the action actually taken divided by the probability of taking that action. The vector is the direction in parameter space that most increases the probability of repeating the action $A_t$ on future visits to state $S_t$. The update increases the parameter vector in this direction proportional to the return, and inversely proportional to the action probability. The former makes sense because it causes the parameter to move most in the directions that favor actions that yield the highest return. The latter makes sense because otherwise actions that are selected frequently are at an advantage (the updates will be more often in their direction) and might win out even if they do not yield the highest return.*\n",
        "\n",
        "$\\;$\n",
        "\n",
        "Sfruttando la legge analitica di derivazione del logaritmo , l'espressione frazionaria viene sostituita da\n",
        "\n",
        "$∇ \\ln \\pi (A_t|S_t, \\theta_t) =  \\frac{\\nabla \\pi (A_t|S_t, \\theta_t)}{\\pi(A_t|S_t, \\theta_t)} $\n",
        "\n",
        "Questa formulazione logaritmica risutla anche computazionalmente più vantaggiosa.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "$\\;$\n",
        "\n",
        "**Convergenza**\n",
        "\n",
        "Per valori di $α$ sufficentemente piccoli,\n",
        "è garantito un miglioramento delle prestazioni della policy nella massimizzazione della ricompensa.\n",
        "\n",
        "La convergenza verso un ottimo locale è garantita con le condizioni di approssimazione stocastica (stochastic approximation conditions) standard, viste più volte in questi notebook, per $α$ decrescente.\n",
        "\n",
        "\n",
        "$\\;$\n",
        "\n",
        "\n",
        "**Svantaggi**\n",
        "\n",
        "\n",
        "Come per il metodo Monte Carlo classico, REINFORCE può presentare un'elevata varianza e quindi produrre un apprendimento lento.\n",
        "\n",
        "Di fattto abbiamo bisogno di ulteriori meccanismi per ridurre la varianza. Un esempio di risposta a tale problema :\n",
        "\n",
        "\n",
        "\n",
        "REINFORCE with Baseline (Sutton & Barto, 2018, PDF 351)\n",
        "\n"
      ],
      "metadata": {
        "id": "4nrDVHLiZeK2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Env: Cart Pole (Gym)\n",
        "\n",
        "$\\;$\n",
        "\n",
        "Nota : In CartPole, l'agente riceve una ricompensa con un valore pari a $+1$ per ogni singolo time-step in cui riesce a mantenere l'equilibrio .\n",
        "\n",
        "https://www.gymlibrary.dev/environments/classic_control/cart_pole/\n",
        "\n"
      ],
      "metadata": {
        "id": "W-D7yGghOtof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "\n",
        "env=gym.make('CartPole-v1')\n",
        "#help(env.unwrapped)\n",
        "\n",
        "episodeNumber=2\n",
        "max_timeSteps=3\n",
        "\n",
        "\n",
        "for episodeIndex in range(1,episodeNumber):\n",
        "    initial_state=env.reset()\n",
        "\n",
        "    #print(\"Ep:\",episodeIndex)\n",
        "    #appendedObservations=[]\n",
        "\n",
        "    for timeIndex in range(1,max_timeSteps+1):\n",
        "        print(\"timeIndex=\",timeIndex)\n",
        "\n",
        "        random_action=env.action_space.sample()\n",
        "        print(\"random_action=\",random_action)\n",
        "\n",
        "        observation, reward, done,  info = env.step(random_action)\n",
        "\n",
        "        #appendedObservations.append(observation)\n",
        "\n",
        "        print(\"[Cart Position,Cart Velocity,Pole Angle,Pole Angular Velocity]:\",observation)\n",
        "        print(\"reward=\",reward)\n",
        "        print(\"done=\",done)\n",
        "        print(\"info=\",info)\n",
        "        print(\"\")\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xovkHfpOvek",
        "outputId": "57bda386-2e60-470f-98f1-68b97cbf90f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "timeIndex= 1\n",
            "random_action= 0\n",
            "[Cart Position,Cart Velocity,Pole Angle,Pole Angular Velocity]: [-0.03882672 -0.17139114  0.04385891  0.34949616]\n",
            "reward= 1.0\n",
            "done= False\n",
            "info= {}\n",
            "\n",
            "timeIndex= 2\n",
            "random_action= 1\n",
            "[Cart Position,Cart Velocity,Pole Angle,Pole Angular Velocity]: [-0.04225454  0.02308048  0.05084883  0.07095963]\n",
            "reward= 1.0\n",
            "done= False\n",
            "info= {}\n",
            "\n",
            "timeIndex= 3\n",
            "random_action= 0\n",
            "[Cart Position,Cart Velocity,Pole Angle,Pole Angular Velocity]: [-0.04179293 -0.1727322   0.05226803  0.37924212]\n",
            "reward= 1.0\n",
            "done= False\n",
            "info= {}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Moduli ed Inizializzazione ambiente"
      ],
      "metadata": {
        "id": "PCQDs_HibFNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = (16, 10)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "torch.manual_seed(0)\n",
        "\n",
        "import base64, io\n",
        "\n",
        "# For visualization\n",
        "from gym.wrappers.monitoring import video_recorder\n",
        "from IPython.display import HTML\n",
        "from IPython import display\n",
        "import glob\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device\n",
        "\n",
        "env = gym.make('CartPole-v1')"
      ],
      "metadata": {
        "id": "x82A8OrsML9i"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Premesse tecniche alla Policy come ANN\n",
        "\n",
        "La **funzione softmax** trasforma un vettore di numeri reali in una distribuzione di probabilità.\n",
        "In pratica, applica una funzione esponenziale a ciascun elemento, rendendolo positivo, e poi divide per la somma di tutti i valori esponenzializzati, garantendo che la somma delle probabilità risulti pari a uno.\n",
        "\n",
        "In questo scenario, dove si sono solo due azioni discrete (le nostre \"classi\"), la funzione softmax produce una distribuzione di probabilità su queste due azioni.\n",
        "\n",
        "Per spazi di azioni discreti, la funzione softmax rappresenta una scelta comune."
      ],
      "metadata": {
        "id": "M5NKE1oDoxAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Funzione softmax in Pytorch\n",
        "# dim (int) – A dimension along which Softmax will be computed (so every slice along dim will sum to 1)\n",
        "\n",
        "softmax0 = torch.nn.Softmax(dim=0) # Applies along columns\n",
        "softmax1 = torch.nn.Softmax(dim=1) # Applies along rows\n",
        "\n",
        "v = np.array([[1,2,3],\n",
        "              [1,5,0]])\n",
        "v =  torch.from_numpy(v).float()\n",
        "\n",
        "print(softmax0(v))\n",
        "print(\"\")\n",
        "print(softmax1(v))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dFHXbyk25jl",
        "outputId": "11f1a4d2-b12f-4ec4-d72f-24c50b1867cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5000, 0.0474, 0.9526],\n",
            "        [0.5000, 0.9526, 0.0474]])\n",
            "\n",
            "tensor([[0.0900, 0.2447, 0.6652],\n",
            "        [0.0179, 0.9756, 0.0066]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Categorical in pytorch\n",
        "\n",
        "#La classe Categorical in PyTorch definisce una distribuzione di probabilità discreta.\n",
        "#Puoi specificare la distribuzione in due modi (ma non contemporaneamente):\n",
        "#Probabilità (probs): Tramite un tensore contenente le probabilità relative di ciascuna categoria. La somma degli elementi di questo tensore deve essere pari a 1 (lungo la dimensione specificata durante il campionamento).\n",
        "#Logit (logits): Tramite un tensore contenente i logit per ciascuna categoria.\n",
        "\n",
        "# Se probs è unidimensionale con lunghezza K, ciascun elemento rappresenta la probabilità relativa di campionare la categoria corrispondente all'indice.\n",
        "# ovvero i campioni saranno interi da {0,…,K−1}\n",
        "\n",
        "#https://pytorch.org/docs/stable/distributions.html\n",
        "\n",
        "#log_prob\n",
        "#Restituisce semplicemente il logaritmo della probabilità che si verifichi un determinato campione (o categoria) estratto dalla distribuzione.\n",
        "#https://en.wikipedia.org/wiki/Log_probability\n",
        "#https://chrispiech.github.io/probabilityForComputerScientists/en/part1/log_probabilities/\n",
        "\n",
        "print(\"E1\")\n",
        "m = Categorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))\n",
        "s = m.sample()\n",
        "print(s)  # ha un uguale probabilità di campionare 0, 1, 2, 3\n",
        "print(m.log_prob(s))\n",
        "print(m.log_prob(torch.from_numpy(np.ones(1)*3)))\n",
        "\n",
        "print(\"E2\")\n",
        "m = Categorical(torch.tensor([ 0.9999, 0.00001 ]))\n",
        "s = m.sample()\n",
        "print(s)  # quasi sempre campiona 0\n",
        "print(m.log_prob(s))\n",
        "print(m.log_prob(torch.from_numpy(np.ones(1))))\n",
        "\n",
        "print(\"E3\")\n",
        "m = Categorical(torch.tensor([ 0.33, 0.33, 0.33, 0.01 ]))\n",
        "s = m.sample()\n",
        "print(s)  # di rado campiona 3\n",
        "print(m.log_prob(s))\n",
        "print(m.log_prob(torch.from_numpy(np.ones(1)*3)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgiKyfKo5kPx",
        "outputId": "b96b3101-78bc-4562-c172-322f3d0501b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E1\n",
            "tensor(0)\n",
            "tensor(-1.3863)\n",
            "tensor([-1.3863])\n",
            "E2\n",
            "tensor(0)\n",
            "tensor(-1.0014e-05)\n",
            "tensor([-11.5128])\n",
            "E3\n",
            "tensor(1)\n",
            "tensor(-1.1087)\n",
            "tensor([-4.6052])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Policy come ANN\n",
        "\n",
        "I metodi basati sulle policy, producono in uscita una probabilità per ogni azione.\n",
        "\n"
      ],
      "metadata": {
        "id": "TlFj5UzVbIR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Codice ispirato :\n",
        "#https://github.com/goodboychan?tab=repositories\n",
        "\n",
        "class Policy(nn.Module):\n",
        "\n",
        "    #Definiamo una ANN a due layer fully connected con relative activation function\n",
        "    def __init__(self, state_size=4, action_size=2, hidden_size=32):\n",
        "        super(Policy, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, hidden_size) # Matrice pesi fc1.weight: torch.Size([32, 4])\n",
        "        self.activation1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, action_size) # Matrice pesi fc2.weight: torch.Size([2, 32])\n",
        "        self.activation2 = torch.nn.Softmax(dim=1) # Nota per dim=0 da chiaramente [[1., 1.]]\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = self.fc1(state)\n",
        "        x1 = self.activation1(x)\n",
        "        x2 = self.fc2(x1) #è una roba del tipo tensor([[-0.0344,  0.0977]], grad_fn=<AddmmBackward0>)\n",
        "        x3 = self.activation2(x2)\n",
        "        return x3\n",
        "\n",
        "    # Definiamo un modo per campionare un azione e il log della sua probabilità\n",
        "    def act(self, state):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device) # questo rigo evita il TypeError:\n",
        "        # linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray\n",
        "\n",
        "        probs = self.forward(state).cpu()\n",
        "        model = Categorical(probs)\n",
        "        action = model.sample() # qui campiona l'azione usando i risultati della ANN\n",
        "        return action.item(), model.log_prob(action)"
      ],
      "metadata": {
        "id": "tcwqe0gIbH6P"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing della classe\n",
        "\n",
        "p = Policy().to(device)\n",
        "state = env.reset()\n",
        "\n",
        "for timeIndex in range(1,2):\n",
        "\n",
        "        state1 = torch.from_numpy(state).float().unsqueeze(0)\n",
        "        print(\"p.forward(state)=\", p.forward(state1))\n",
        "\n",
        "        action, log_prob = p.act(state)\n",
        "        print(\"action=\",action)\n",
        "        print(\"log_prob=\",log_prob)\n",
        "\n",
        "        state, reward, done, _ = env.step(action)\n",
        "\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99e1c770-fc16-4a10-eb6b-210f7540284e",
        "id": "UUfSutVMwR6l"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p.forward(state)= tensor([[0.5816, 0.4184]], grad_fn=<SoftmaxBackward0>)\n",
            "action= 0\n",
            "log_prob= tensor([-0.5419], grad_fn=<SqueezeBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Premesse alla REINFORCE Core function\n"
      ],
      "metadata": {
        "id": "iKBt4S-CCeqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Premssa Tecnica : torch.cat\n",
        "#https://pytorch.org/docs/stable/generated/torch.cat.html\n",
        "\n",
        "x = torch.randn(1, )\n",
        "print(x)\n",
        "y = torch.randn(1, )\n",
        "print(y)\n",
        "\n",
        "print(torch.cat([x,y],dim=0))\n",
        "print( (torch.cat([x,y],dim=0))[0].type)\n",
        "print(\"sum=\", torch.cat([x,y]).sum())\n",
        "print(\"\")\n",
        "\n",
        "####\n",
        "\n",
        "x = torch.randn(1,4 )\n",
        "print(x)\n",
        "print(torch.cat([x, x, x],dim=1))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIShOYzZHfoo",
        "outputId": "8ca250db-028b-4866-f3b4-422d6bb8ca76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.3001])\n",
            "tensor([1.3226])\n",
            "tensor([0.3001, 1.3226])\n",
            "<built-in method type of Tensor object at 0x798f25ca7330>\n",
            "sum= tensor(1.6227)\n",
            "\n",
            "tensor([[-1.2141, -2.2789, -2.6911,  0.0192]])\n",
            "tensor([[-1.2141, -2.2789, -2.6911,  0.0192, -1.2141, -2.2789, -2.6911,  0.0192,\n",
            "         -1.2141, -2.2789, -2.6911,  0.0192]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Buono a sapersi:\n",
        "\n",
        "Anche se non è il caso di Cart Pole, dove tutti i rewads sono uguali (e non verrà qui applicato); generalmente, per questioni di stabilità, si preferisce normalizzare il return dei rewards. Se si analizza le equazioni di backpropagation, si osserva che il return influenza i gradienti. Per tale ragione, si vuole mantenere i valori campionati del return, in un intervallo specifico.\n",
        "\n",
        "L' adozione di questa normalizzazione non si basa su solide garanzie teoriche, bensì su considerazioni di natura pratiaca."
      ],
      "metadata": {
        "id": "Xtw5eSbZ3kVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_rewards(r):\n",
        "  # r è la lista contenente i rewads accumulati dell'episodio\n",
        "  r = torch.tensor(r).to(device)\n",
        "  print(r.mean())\n",
        "  print(r.std())\n",
        "  r = (r - r.mean()) / (r.std() + 1e-9)\n",
        "  return r # ritorno la lista di reward normalizzati"
      ],
      "metadata": {
        "id": "hLi2D03v3gKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### REINFORCE Core function"
      ],
      "metadata": {
        "id": "Mz_O9I7R4K-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def list_G_for_each_step(input_list, gamma=1):\n",
        "    output_list = []\n",
        "    # Itera in reverse order\n",
        "    for i in range(len(input_list) - 1, -1, -1):\n",
        "        # Calcola la somma\n",
        "        current_sum = sum(input_list[i:])\n",
        "        output_list.append(current_sum)\n",
        "        # NOTA: semplifichiamo le cose assumendo che gamma = 1.\n",
        "        # Altrimenti nell'elaborazione avremmo dovuto considerare anche\n",
        "        # il fattore discount:  *gamma**(k-t-1))\n",
        "        # Vedi algoritmo di Sutton&Barto PDF 350\n",
        "    output_list.reverse()\n",
        "    return output_list\n",
        "\n",
        "\n",
        "def reinforce(policy, optimizer, n_episodes=1000, max_t=1000, gamma=1.0, print_every=100):\n",
        "\n",
        "    scores_deque = deque(maxlen=100)\n",
        "    scores = []\n",
        "\n",
        "    for e in range(1, n_episodes+1):\n",
        "        saved_log_probs = []\n",
        "        rewards = []\n",
        "        state = env.reset()\n",
        "\n",
        "        # Genera una traiettoria\n",
        "        for t in range(max_t):\n",
        "            action, log_prob = policy.act(state)\n",
        "            saved_log_probs.append(log_prob) # è roba del tipo : log_prob= tensor([-0.7196])\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            rewards.append(reward)\n",
        "            #la lista reward cresce in dimensione ad ogni step: rewards [1.0], rewards [1.0, 1.0], rewards [1.0, 1.0, 1.0], ...\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "\n",
        "        #Elabora il return G_t\n",
        "        G = list_G_for_each_step(rewards)\n",
        "        #R = sum(rewards)  # un alternativa praticabile\n",
        "\n",
        "        # Utility\n",
        "        scores_deque.append(sum(rewards)) # Serve per il debug\n",
        "        scores.append(sum(rewards)) # Serve per il grafico\n",
        "\n",
        "        # Calcola la loss\n",
        "        policy_loss = []\n",
        "        for j in range(len(G)):\n",
        "            # Tieni presente che stiamo utilizzando Gradient Ascent, non il Descent.\n",
        "            # E' una convenzione standard, sia in PyTorch che TF, eseguire la minimizzazione anziché la massimizzazione.\n",
        "            # Quindi, dobbiamo convertire l'obiettivo da massimizzazione in obiettivo da minimizzazione,\n",
        "            # semplicemente aggiungendo un segno negativo.\n",
        "            policy_loss.append(-saved_log_probs[j] * G[j])\n",
        "\n",
        "        # Varainte\n",
        "        # A differenza dell'algortimo proposto da Sutton PDF 350\n",
        "        # dove update dei parametri è fatto \"for each step of the episode\"\n",
        "        # in questo caso  preferisco fare un unico update per ogni episodio\n",
        "        # sfruttando la somma sulle (log_prod*G) campionate\n",
        "        # Nota, usando: torch.cat(policy_loss).mean() le cose non funzionano,\n",
        "        # ottengo valori ballerini Average Score: 75.22,.. 493.66, 177.89 , ... , 91.63\n",
        "        # e risulta nei test : \"Max reward was taken in  0.0 % of times\"\n",
        "        #\n",
        "        # Si tenga presente che torch.cat\n",
        "        # trasforma la lista classica di tensori in un tensore \"lista\" che contiene tensori\n",
        "        # cioè, da: policy_loss= [tensor([12.8589], grad_fn=<MulBackward0>), tensor([12.5127], grad_fn=<MulBackward0>), tensor([8.6259],..]\n",
        "        # a:  policy_loss= tensor([12.8589, 12.5127, 8.6259, ...,  12.4928])\n",
        "        policy_loss = torch.cat(policy_loss).sum()\n",
        "        #policy_loss è una roba del tipo : tensor(10.4294, grad_fn=<MeanBackward0>)\n",
        "\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        # Debug\n",
        "        if e % print_every == 0:\n",
        "            print('Episode {}\\tAverage Score: {:.2f}'.format(e, np.mean(scores_deque)))\n",
        "\n",
        "        # Se raggiungi un certo punteggio, esempio 195.0, ferma il processo di training\n",
        "        #if np.mean(scores_deque) >= 195.0:\n",
        "        #    print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(e - 100, np.mean(scores_deque)))\n",
        "        #    break\n",
        "\n",
        "    print(\"\\n...End of the training.\\n\")\n",
        "    return scores"
      ],
      "metadata": {
        "id": "73f5baxkDBTE"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "dbvhfbCIelAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.autograd.set_detect_anomaly(True)\n",
        "#env.spec.reward_threshold = 600  # the standard threshold for rewards is 500 for v1\n",
        "\n",
        "policy = Policy().to(device)\n",
        "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
        "score = reinforce(policy, optimizer, n_episodes=1_000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_2MwvsXLBy7",
        "outputId": "7327b5f8-ce52-4315-cd6a-2632b115a4c9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 100\tAverage Score: 53.81\n",
            "Episode 200\tAverage Score: 204.24\n",
            "Episode 300\tAverage Score: 266.08\n",
            "Episode 400\tAverage Score: 153.34\n",
            "Episode 500\tAverage Score: 281.38\n",
            "Episode 600\tAverage Score: 258.84\n",
            "Episode 700\tAverage Score: 255.41\n",
            "Episode 800\tAverage Score: 422.23\n",
            "Episode 900\tAverage Score: 494.36\n",
            "Episode 1000\tAverage Score: 500.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Plot"
      ],
      "metadata": {
        "id": "hkJBCeeEjwdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the scores\n",
        "fig = plt.figure(figsize=(4,2))\n",
        "#ax = fig.add_subplot(111)\n",
        "ax = plt.gca()\n",
        "ax.set_ylim([1, 600])\n",
        "plt.plot(np.arange(1, len(score)+1), score)\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Episode')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "m_8UQxeSjqa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test con Video"
      ],
      "metadata": {
        "id": "jprU4k4QkLc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_video(env_name):\n",
        "    mp4list = glob.glob('./*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = './{}.mp4'.format(env_name)\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        display.display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "def show_video_of_model(policy, env_name):\n",
        "    env = gym.make(env_name)\n",
        "    vid = video_recorder.VideoRecorder(env, path=\"./{}.mp4\".format(env_name))\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    for t in range(1000):\n",
        "        vid.capture_frame()\n",
        "        action, _ = policy.act(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        state = next_state\n",
        "        if done:\n",
        "            break\n",
        "    vid.close()\n",
        "    env.close()"
      ],
      "metadata": {
        "id": "7RO2VEqokRxc"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_video_of_model(policy, 'CartPole-v1')"
      ],
      "metadata": {
        "id": "_qO1FxNNkVTQ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_video('CartPole-v1')"
      ],
      "metadata": {
        "id": "zNvm98ziLJ0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test su 100 campioni"
      ],
      "metadata": {
        "id": "wBaxTF2BnY0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "episodeNumber=100\n",
        "max_timeSteps=1_000\n",
        "\n",
        "cont = 0\n",
        "max_reward = 500\n",
        "\n",
        "\n",
        "for episodeIndex in range(1,episodeNumber):\n",
        "    observation=env.reset()\n",
        "    rewards = 0\n",
        "\n",
        "    for timeIndex in range(1,max_timeSteps+1):\n",
        "        action, _ = policy.act(observation)\n",
        "        observation, reward, done,  info = env.step(action)\n",
        "        rewards += reward\n",
        "\n",
        "        if done:\n",
        "            if max_reward == rewards:\n",
        "                  cont += 1\n",
        "\n",
        "            break\n",
        "\n",
        "env.close()\n",
        "\n",
        "print(\"Max reward was taken in \", round((cont/episodeNumber)*episodeNumber,2), \"% of times\" )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jO-HG2hknesH",
        "outputId": "6cf642e1-61ee-4f29-ceca-803b510b262b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max reward was taken in  99.0 % of times\n"
          ]
        }
      ]
    }
  ]
}